{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Nascon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNdbDBDXBUBd",
        "colab_type": "code",
        "outputId": "20b420eb-25f6-4418-c0c1-efa6c11205d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgx2KnCxBzvG",
        "colab_type": "code",
        "outputId": "75639a64-e282-4266-b1a4-bcf94c192450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!mkdir DataSet\n",
        "!unzip ./gdrive/My\\ Drive/nascon/Train.csv.zip -d DataSet\n",
        "!unzip ./gdrive/My\\ Drive/nascon/Test.csv.zip -d DataSet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./gdrive/My Drive/nascon/Train.csv.zip\n",
            "  inflating: DataSet/Train.csv       \n",
            "Archive:  ./gdrive/My Drive/nascon/Test.csv.zip\n",
            "  inflating: DataSet/Test.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-IHrvmXC7ci",
        "colab_type": "code",
        "outputId": "65ea5458-43c3-486c-86c9-733178468f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls DataSet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test.csv  Train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxO_2aahCJP_",
        "colab_type": "code",
        "outputId": "e2ae624e-a0e1-406d-f63b-9ec3d2362c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%pylab inline\n",
        "import scipy.stats\n",
        "from collections import defaultdict\n",
        "plt.style.use('ggplot')\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "%load_ext autoreload \n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D, Conv3D, MaxPooling3D, ZeroPadding3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDv8_VOgCjPE",
        "colab_type": "code",
        "outputId": "321e76e7-dae3-4fa6-eaa1-90e5dcd085d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1047
        }
      },
      "source": [
        "data=pd.read_csv('./DataSet/Train.csv')\n",
        "print data.describe()\n",
        "print data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              raw_0         raw_1         raw_2         raw_3         raw_4  \\\n",
            "count  46134.000000  46245.000000  46107.000000  46078.000000  46042.000000   \n",
            "mean     123.913773    129.869781    121.405557    135.749078    118.019113   \n",
            "std       62.177844     67.735543     59.393345     66.367796     58.679947   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%       77.000000     77.000000     78.000000     84.000000     74.000000   \n",
            "50%      122.000000    128.000000    118.000000    134.000000    116.000000   \n",
            "75%      168.000000    181.000000    161.000000    187.000000    158.000000   \n",
            "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
            "\n",
            "              raw_5         raw_6         raw_7         raw_8         raw_9  \\\n",
            "count  46108.000000  46087.000000  46181.000000  46089.000000  46110.000000   \n",
            "mean     119.762449    132.968147    134.228102    123.466532    134.038213   \n",
            "std       59.169917     63.933190     65.342831     61.314987     65.576536   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%       77.000000     84.000000     84.000000     77.000000     84.000000   \n",
            "50%      117.000000    131.000000    133.000000    121.000000    132.000000   \n",
            "75%      159.000000    181.000000    184.000000    167.000000    184.000000   \n",
            "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
            "\n",
            "           ...        sparse_1015  sparse_1016   sparse_1017   sparse_1018  \\\n",
            "count      ...       24272.000000  24385.00000  24147.000000  24140.000000   \n",
            "mean       ...         125.135794    125.05487    125.156707    125.228873   \n",
            "std        ...          60.610376     61.00753     60.909433     60.916934   \n",
            "min        ...           0.000000      0.00000      0.000000      0.000000   \n",
            "25%        ...          81.000000     81.00000     81.000000     81.000000   \n",
            "50%        ...         123.000000    122.00000    123.000000    123.000000   \n",
            "75%        ...         166.000000    167.00000    167.000000    166.000000   \n",
            "max        ...         255.000000    255.00000    255.000000    255.000000   \n",
            "\n",
            "        sparse_1019   sparse_1020   sparse_1021   sparse_1022   sparse_1023  \\\n",
            "count  24025.000000  24082.000000  24384.000000  24434.000000  24213.000000   \n",
            "mean     125.186847    124.705132    124.677616    124.819514    124.156486   \n",
            "std       60.963374     60.945215     60.952477     61.661404     61.954084   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%       81.000000     80.000000     80.000000     80.000000     79.000000   \n",
            "50%      123.000000    122.000000    122.000000    122.000000    121.000000   \n",
            "75%      166.000000    165.750000    165.000000    167.000000    165.000000   \n",
            "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
            "\n",
            "              label  \n",
            "count  48500.000000  \n",
            "mean       4.500392  \n",
            "std        2.872828  \n",
            "min        0.000000  \n",
            "25%        2.000000  \n",
            "50%        5.000000  \n",
            "75%        7.000000  \n",
            "max        9.000000  \n",
            "\n",
            "[8 rows x 1393 columns]\n",
            "Index([u'raw_0', u'raw_1', u'raw_2', u'raw_3', u'raw_4', u'raw_5', u'raw_6',\n",
            "       u'raw_7', u'raw_8', u'raw_9',\n",
            "       ...\n",
            "       u'sparse_1015', u'sparse_1016', u'sparse_1017', u'sparse_1018',\n",
            "       u'sparse_1019', u'sparse_1020', u'sparse_1021', u'sparse_1022',\n",
            "       u'sparse_1023', u'label'],\n",
            "      dtype='object', length=1393)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJxz_XqkDACp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndiqS7kFWxCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('./DataSet/Train.csv')\n",
        "test=pd.read_csv('./DataSet/Test.csv')\n",
        "normalized_train=train.copy()\n",
        "normalized_train=normalized_train.drop(normalized_train.iloc[:,368:1392],axis=1)\n",
        "normalized_train=normalized_train.interpolate()\n",
        "normalized_train=normalized_train.dropna()\n",
        "labels=normalized_train.label\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM-XdhQkW7zk",
        "colab_type": "code",
        "outputId": "47e1046d-898f-4cca-9d4a-0df720995d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ndf=normalized_train.iloc[:,0:368].copy()\n",
        "ndf = np.array(ndf)\n",
        "ndfs = np.square(ndf)\n",
        "ndfc = np.power(ndf,3)\n",
        "ndf = np.hstack((ndf,ndfs,ndfc))\n",
        "print ndf.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48498, 1104)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZRqo4MdXWDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(ndf,labels,test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkHFZZROW8Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "polynomial_features= PolynomialFeatures(degree=2)\n",
        "x_poly = polynomial_features.fit_transform(X_train)\n",
        "model = LinearRegression()\n",
        "model.fit(x_poly, Y_train)\n",
        "# y_poly_pred = model.predict(x_poly)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjAeZzSrX_WM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlF4hnAIEFNm",
        "colab_type": "code",
        "outputId": "2a3d84b0-8577-4507-cbde-bee1f656b7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        }
      },
      "source": [
        "#First plot the frequency of each class...\n",
        "freq=scipy.stats.itemfreq(Y)# Get frequency of each class....\n",
        "plt.bar(np.arange(1,len(freq[:,1])+1),freq[:,1],width=0.35)\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of different class examples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `itemfreq` is deprecated!\n",
            "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Frequency of different class examples')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnEAAAH0CAYAAABSGHvOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nykwLuxVdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPoe8xbzETwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data in matrix\n",
        "# X=np.asarray(data.ix[:,1:-1].dropna())\n",
        "# print X.shape\n",
        "# Y=np.asarray(data.ix[:,-1])\n",
        "# print Y, Y.shape, len(np.unique(Y))\n",
        "# data = data.fillna(data.mean(axis=0))\n",
        "from sklearn.model_selection import train_test_split\n",
        "data=data.drop(data.iloc[:,368:1392],axis=1)\n",
        "data=data.interpolate()\n",
        "# normalized_train=normalized_train.dropna(\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:368], Y, test_size=0.33)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIABS7BFE-cU",
        "colab_type": "code",
        "outputId": "d79f5489-544a-4aef-9974-9de7a56f1768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "print X_train.columns\n",
        "print X_train.shape\n",
        "X_train=np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "img_rows = 16\n",
        "img_cols = 23\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index([u'raw_0', u'raw_1', u'raw_2', u'raw_3', u'raw_4', u'raw_5', u'raw_6',\n",
            "       u'raw_7', u'raw_8', u'raw_9',\n",
            "       ...\n",
            "       u'GLCM_14', u'GLCM_15', u'GLCM_16', u'GLCM_17', u'GLCM_18', u'GLCM_19',\n",
            "       u'GLCM_20', u'GLCM_21', u'GLCM_22', u'GLCM_23'],\n",
            "      dtype='object', length=368)\n",
            "(32495, 368)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aXRhQ-2vjMN",
        "colab_type": "code",
        "outputId": "9b5bb38b-8197-4c62-a812-38cdecb9893e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "a = X_train[5].reshape(32,32)\n",
        "b = X_train[2].reshape(32,32)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(a, interpolation='nearest')\n",
        "print plt.show()\n",
        "plt.imshow(b, interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAHVCAYAAABSR+pHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFyHXao0Su7jtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAHVCAYAAABSR+pHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3TuiVhKpjOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "# for i in range(X_train.shape[0]):\n",
        "#   f = X_train[i].reshape(32,32)\n",
        "#   X.append(f)\n",
        "# X = np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_DNJ8NqyWA8",
        "colab_type": "code",
        "outputId": "3f1807f2-216d-4217-a443-e2bb9a71bc6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osPyQOgTpnsq",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F87NbvaFG9P",
        "colab_type": "code",
        "outputId": "f8138bb9-e1c5-407e-ec1c-10661677f38b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, Reshape\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "\n",
        "print model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 14, 21, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 12, 19, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 6, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 6, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               442496    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 462,602\n",
            "Trainable params: 462,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsKCouKBlyMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NV-z7bwuKYo",
        "colab_type": "code",
        "outputId": "3314ae38-6118-4300-e99c-032dc38d4785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "\n",
        "print y_train.shape,X_train.shape\n",
        "# adam = keras.optimizers.Adam(lr = 0.00001)\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test,y_test), shuffle=True)\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='rmsprop',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model.fit(X_train, y_train, batch_size=16, epochs=10)\n",
        "# score = model.evaluate(x_test, y_test, batch_size=16)\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32495, 10) (32495, 16, 23, 1)\n",
            "Train on 32495 samples, validate on 16005 samples\n",
            "Epoch 1/10\n",
            "32495/32495 [==============================] - 14s 427us/step - loss: 2.1742 - acc: 0.1890 - val_loss: 2.3037 - val_acc: 0.0973\n",
            "Epoch 2/10\n",
            "32495/32495 [==============================] - 11s 345us/step - loss: 2.3030 - acc: 0.1013 - val_loss: 2.3031 - val_acc: 0.0973\n",
            "Epoch 3/10\n",
            " 7168/32495 [=====>........................] - ETA: 7s - loss: 2.3025 - acc: 0.1002"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-91efcf7e6474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# model.fit(X_train, y_train, batch_size=16, epochs=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# score = model.evaluate(x_test, y_test, batch_size=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYewiXFmvA9z",
        "colab_type": "code",
        "outputId": "ba5ad97e-6c3e-4b15-b39b-35e856ab82fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.predict_classes(X_test)\n",
        "# pred = np.argmax(results, axis=1)\n",
        "y = np.argmax(y_test, axis=1)\n",
        "print np.sum(results==y)/float(len(y))\n",
        "# score, acc = model.evaluate(X_test, y_test, batch_size=16)\n",
        "# print('Test score:', score)\n",
        "# print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.408622305529522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyhd7cbE1EQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZUFa96abAkG",
        "colab_type": "text"
      },
      "source": [
        "# GradBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcbMUazNbH7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds_xh9UnbZMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('./DataSet/Train.csv')\n",
        "test=pd.read_csv('./DataSet/Test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlbUgNggbh9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_train=train.copy()\n",
        "normalized_train=normalized_train.drop(normalized_train.iloc[:,368:1392],axis=1)\n",
        "normalized_train=normalized_train.interpolate()\n",
        "normalized_train=normalized_train.dropna()\n",
        "labels=normalized_train.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qTNrCjTN2RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bH-u_DHb2gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(normalized_train.ilo[:,0:368],labels,test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAlNK36icLy_",
        "colab_type": "code",
        "outputId": "a2bcb955-3be0-4d1b-b178-ec9b62f01fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgbcl = XGBClassifier(n_estimators=100,max_depth=4,verbosity=3,n_jobs=-1,learning_rate=0.5)\n",
        "xgbcl.fit(X_train, Y_train)\n",
        "pred=xgbcl.predict(X_test)\n",
        "print sklearn.metrics.accuracy_score(Y_test.values,pred)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5285910652920962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF75Lz6ApOzl",
        "colab_type": "code",
        "outputId": "0b37edcb-97fd-4673-fc74-c2c592fbb780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "normalized_train.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([u'raw_0', u'raw_1', u'raw_2', u'raw_3', u'raw_4', u'raw_5', u'raw_6',\n",
              "       u'raw_7', u'raw_8', u'raw_9',\n",
              "       ...\n",
              "       u'GLCM_15', u'GLCM_16', u'GLCM_17', u'GLCM_18', u'GLCM_19', u'GLCM_20',\n",
              "       u'GLCM_21', u'GLCM_22', u'GLCM_23', u'label'],\n",
              "      dtype='object', length=369)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O9hEwHrnqCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ndf=normalized_train.iloc[:,0:368].copy()\n",
        "ndf['xg']=xgbcl.predict(ndf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ZeAxR4phgM",
        "colab_type": "code",
        "outputId": "d65d5915-2c49-4345-f96b-af018a6c65de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1989
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_0</th>\n",
              "      <th>raw_1</th>\n",
              "      <th>raw_2</th>\n",
              "      <th>raw_3</th>\n",
              "      <th>raw_4</th>\n",
              "      <th>raw_5</th>\n",
              "      <th>raw_6</th>\n",
              "      <th>raw_7</th>\n",
              "      <th>raw_8</th>\n",
              "      <th>raw_9</th>\n",
              "      <th>...</th>\n",
              "      <th>GLCM_15</th>\n",
              "      <th>GLCM_16</th>\n",
              "      <th>GLCM_17</th>\n",
              "      <th>GLCM_18</th>\n",
              "      <th>GLCM_19</th>\n",
              "      <th>GLCM_20</th>\n",
              "      <th>GLCM_21</th>\n",
              "      <th>GLCM_22</th>\n",
              "      <th>GLCM_23</th>\n",
              "      <th>xg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>249.0</td>\n",
              "      <td>250.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.790219</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>319.633333</td>\n",
              "      <td>12.100000</td>\n",
              "      <td>0.088411</td>\n",
              "      <td>0.049826</td>\n",
              "      <td>0.855383</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>126.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>243.5</td>\n",
              "      <td>120.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050260</td>\n",
              "      <td>0.851072</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>236.862500</td>\n",
              "      <td>11.520833</td>\n",
              "      <td>0.094054</td>\n",
              "      <td>0.050346</td>\n",
              "      <td>0.789885</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>226.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>231.0</td>\n",
              "      <td>231.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050346</td>\n",
              "      <td>0.875646</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>720.466667</td>\n",
              "      <td>16.183333</td>\n",
              "      <td>0.133303</td>\n",
              "      <td>0.051539</td>\n",
              "      <td>0.890142</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>169.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.711023</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>433.141667</td>\n",
              "      <td>13.550000</td>\n",
              "      <td>0.101400</td>\n",
              "      <td>0.049036</td>\n",
              "      <td>0.902981</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>154.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050260</td>\n",
              "      <td>0.852472</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>325.300000</td>\n",
              "      <td>12.533333</td>\n",
              "      <td>0.122460</td>\n",
              "      <td>0.050173</td>\n",
              "      <td>0.910305</td>\n",
              "      <td>0.002517</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>77.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047690</td>\n",
              "      <td>0.856955</td>\n",
              "      <td>0.002274</td>\n",
              "      <td>1002.316667</td>\n",
              "      <td>23.733333</td>\n",
              "      <td>0.058546</td>\n",
              "      <td>0.047871</td>\n",
              "      <td>0.844166</td>\n",
              "      <td>0.002292</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>104.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053359</td>\n",
              "      <td>0.888191</td>\n",
              "      <td>0.002847</td>\n",
              "      <td>185.091667</td>\n",
              "      <td>9.266667</td>\n",
              "      <td>0.112188</td>\n",
              "      <td>0.049826</td>\n",
              "      <td>0.901720</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>97.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>251.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048143</td>\n",
              "      <td>0.853129</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>1013.541667</td>\n",
              "      <td>17.600000</td>\n",
              "      <td>0.226261</td>\n",
              "      <td>0.092327</td>\n",
              "      <td>0.924492</td>\n",
              "      <td>0.008524</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>224.0</td>\n",
              "      <td>227.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>214.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>215.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>214.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048052</td>\n",
              "      <td>0.869933</td>\n",
              "      <td>0.002309</td>\n",
              "      <td>1220.070833</td>\n",
              "      <td>21.679167</td>\n",
              "      <td>0.114511</td>\n",
              "      <td>0.048858</td>\n",
              "      <td>0.798455</td>\n",
              "      <td>0.002387</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>153.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056596</td>\n",
              "      <td>0.927067</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>1356.254167</td>\n",
              "      <td>19.379167</td>\n",
              "      <td>0.174106</td>\n",
              "      <td>0.058704</td>\n",
              "      <td>0.869596</td>\n",
              "      <td>0.003446</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>82.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051201</td>\n",
              "      <td>0.837024</td>\n",
              "      <td>0.002622</td>\n",
              "      <td>1488.866667</td>\n",
              "      <td>22.566667</td>\n",
              "      <td>0.107707</td>\n",
              "      <td>0.056519</td>\n",
              "      <td>0.694368</td>\n",
              "      <td>0.003194</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>105.5</td>\n",
              "      <td>40.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053765</td>\n",
              "      <td>0.929167</td>\n",
              "      <td>0.002891</td>\n",
              "      <td>637.754167</td>\n",
              "      <td>13.487500</td>\n",
              "      <td>0.184824</td>\n",
              "      <td>0.054566</td>\n",
              "      <td>0.873465</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>129.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048143</td>\n",
              "      <td>0.829038</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>227.700000</td>\n",
              "      <td>11.708333</td>\n",
              "      <td>0.082822</td>\n",
              "      <td>0.048233</td>\n",
              "      <td>0.917249</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>123.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064077</td>\n",
              "      <td>0.851049</td>\n",
              "      <td>0.004106</td>\n",
              "      <td>125.158333</td>\n",
              "      <td>7.383333</td>\n",
              "      <td>0.208491</td>\n",
              "      <td>0.059293</td>\n",
              "      <td>0.878872</td>\n",
              "      <td>0.003516</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>38.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>95.5</td>\n",
              "      <td>83.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049036</td>\n",
              "      <td>0.889924</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>481.875000</td>\n",
              "      <td>12.666667</td>\n",
              "      <td>0.152741</td>\n",
              "      <td>0.055979</td>\n",
              "      <td>0.852423</td>\n",
              "      <td>0.003134</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>106.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>119.5</td>\n",
              "      <td>91.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049564</td>\n",
              "      <td>0.737556</td>\n",
              "      <td>0.002457</td>\n",
              "      <td>383.187500</td>\n",
              "      <td>13.445833</td>\n",
              "      <td>0.102674</td>\n",
              "      <td>0.050775</td>\n",
              "      <td>0.775500</td>\n",
              "      <td>0.002578</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>169.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047690</td>\n",
              "      <td>0.846624</td>\n",
              "      <td>0.002274</td>\n",
              "      <td>1476.270833</td>\n",
              "      <td>25.670833</td>\n",
              "      <td>0.072999</td>\n",
              "      <td>0.050861</td>\n",
              "      <td>0.612740</td>\n",
              "      <td>0.002587</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>179.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>196.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>189.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060524</td>\n",
              "      <td>0.869558</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>362.629167</td>\n",
              "      <td>12.887500</td>\n",
              "      <td>0.103488</td>\n",
              "      <td>0.049213</td>\n",
              "      <td>0.908461</td>\n",
              "      <td>0.002422</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>58.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050604</td>\n",
              "      <td>0.879202</td>\n",
              "      <td>0.002561</td>\n",
              "      <td>244.470833</td>\n",
              "      <td>11.704167</td>\n",
              "      <td>0.111373</td>\n",
              "      <td>0.051539</td>\n",
              "      <td>0.612850</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>88.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047416</td>\n",
              "      <td>0.719149</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>563.062500</td>\n",
              "      <td>17.537500</td>\n",
              "      <td>0.087897</td>\n",
              "      <td>0.048947</td>\n",
              "      <td>0.901529</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>52.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060668</td>\n",
              "      <td>0.952798</td>\n",
              "      <td>0.003681</td>\n",
              "      <td>318.991667</td>\n",
              "      <td>10.150000</td>\n",
              "      <td>0.212709</td>\n",
              "      <td>0.063942</td>\n",
              "      <td>0.857735</td>\n",
              "      <td>0.004089</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>110.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056826</td>\n",
              "      <td>0.781407</td>\n",
              "      <td>0.003229</td>\n",
              "      <td>474.608333</td>\n",
              "      <td>12.150000</td>\n",
              "      <td>0.166958</td>\n",
              "      <td>0.057130</td>\n",
              "      <td>0.838909</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>169.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>251.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>206.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052540</td>\n",
              "      <td>0.790246</td>\n",
              "      <td>0.002760</td>\n",
              "      <td>769.670833</td>\n",
              "      <td>17.354167</td>\n",
              "      <td>0.096294</td>\n",
              "      <td>0.049036</td>\n",
              "      <td>0.779746</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>108.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>249.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>198.5</td>\n",
              "      <td>254.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048681</td>\n",
              "      <td>0.625437</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>346.966667</td>\n",
              "      <td>12.508333</td>\n",
              "      <td>0.105584</td>\n",
              "      <td>0.050946</td>\n",
              "      <td>0.705397</td>\n",
              "      <td>0.002595</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>189.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>191.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052457</td>\n",
              "      <td>0.829801</td>\n",
              "      <td>0.002752</td>\n",
              "      <td>162.579167</td>\n",
              "      <td>9.429167</td>\n",
              "      <td>0.108925</td>\n",
              "      <td>0.051370</td>\n",
              "      <td>0.876130</td>\n",
              "      <td>0.002639</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>173.5</td>\n",
              "      <td>104.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053926</td>\n",
              "      <td>0.890963</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>468.575000</td>\n",
              "      <td>14.166667</td>\n",
              "      <td>0.114212</td>\n",
              "      <td>0.050604</td>\n",
              "      <td>0.878158</td>\n",
              "      <td>0.002561</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>168.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049739</td>\n",
              "      <td>0.753276</td>\n",
              "      <td>0.002474</td>\n",
              "      <td>1152.820833</td>\n",
              "      <td>22.995833</td>\n",
              "      <td>0.064679</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.692666</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>64.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055979</td>\n",
              "      <td>0.902675</td>\n",
              "      <td>0.003134</td>\n",
              "      <td>470.454167</td>\n",
              "      <td>14.362500</td>\n",
              "      <td>0.123147</td>\n",
              "      <td>0.050604</td>\n",
              "      <td>0.799103</td>\n",
              "      <td>0.002561</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>250.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>120.5</td>\n",
              "      <td>246.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047599</td>\n",
              "      <td>0.865270</td>\n",
              "      <td>0.002266</td>\n",
              "      <td>1751.454167</td>\n",
              "      <td>28.370833</td>\n",
              "      <td>0.073667</td>\n",
              "      <td>0.048233</td>\n",
              "      <td>0.815526</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>167.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053522</td>\n",
              "      <td>0.958077</td>\n",
              "      <td>0.002865</td>\n",
              "      <td>331.066667</td>\n",
              "      <td>12.216667</td>\n",
              "      <td>0.098480</td>\n",
              "      <td>0.050260</td>\n",
              "      <td>0.915835</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48470</th>\n",
              "      <td>90.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049564</td>\n",
              "      <td>0.727174</td>\n",
              "      <td>0.002457</td>\n",
              "      <td>157.241667</td>\n",
              "      <td>8.158333</td>\n",
              "      <td>0.184644</td>\n",
              "      <td>0.057509</td>\n",
              "      <td>0.786274</td>\n",
              "      <td>0.003307</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48471</th>\n",
              "      <td>76.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050518</td>\n",
              "      <td>0.754752</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>186.733333</td>\n",
              "      <td>8.400000</td>\n",
              "      <td>0.151110</td>\n",
              "      <td>0.056443</td>\n",
              "      <td>0.800287</td>\n",
              "      <td>0.003186</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48472</th>\n",
              "      <td>45.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.782937</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>496.625000</td>\n",
              "      <td>15.450000</td>\n",
              "      <td>0.061258</td>\n",
              "      <td>0.048858</td>\n",
              "      <td>0.753986</td>\n",
              "      <td>0.002387</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48473</th>\n",
              "      <td>183.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>196.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>127.5</td>\n",
              "      <td>166.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056519</td>\n",
              "      <td>0.881859</td>\n",
              "      <td>0.003194</td>\n",
              "      <td>581.775000</td>\n",
              "      <td>16.833333</td>\n",
              "      <td>0.117120</td>\n",
              "      <td>0.051116</td>\n",
              "      <td>0.819848</td>\n",
              "      <td>0.002613</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48474</th>\n",
              "      <td>254.0</td>\n",
              "      <td>250.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047781</td>\n",
              "      <td>0.753503</td>\n",
              "      <td>0.002283</td>\n",
              "      <td>1447.687500</td>\n",
              "      <td>23.220833</td>\n",
              "      <td>0.097062</td>\n",
              "      <td>0.049476</td>\n",
              "      <td>0.799342</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48475</th>\n",
              "      <td>164.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048143</td>\n",
              "      <td>0.838396</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>555.604167</td>\n",
              "      <td>18.129167</td>\n",
              "      <td>0.066879</td>\n",
              "      <td>0.047507</td>\n",
              "      <td>0.748635</td>\n",
              "      <td>0.002257</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48476</th>\n",
              "      <td>128.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>74.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>86.5</td>\n",
              "      <td>145.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049301</td>\n",
              "      <td>0.900144</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>390.979167</td>\n",
              "      <td>14.654167</td>\n",
              "      <td>0.089177</td>\n",
              "      <td>0.048143</td>\n",
              "      <td>0.911876</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48477</th>\n",
              "      <td>84.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>124.5</td>\n",
              "      <td>113.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056596</td>\n",
              "      <td>0.709470</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>643.887500</td>\n",
              "      <td>17.937500</td>\n",
              "      <td>0.099861</td>\n",
              "      <td>0.049301</td>\n",
              "      <td>0.589718</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48478</th>\n",
              "      <td>109.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>129.5</td>\n",
              "      <td>158.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050346</td>\n",
              "      <td>0.873416</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>95.345833</td>\n",
              "      <td>6.462500</td>\n",
              "      <td>0.189998</td>\n",
              "      <td>0.057206</td>\n",
              "      <td>0.968339</td>\n",
              "      <td>0.003273</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48479</th>\n",
              "      <td>155.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>167.5</td>\n",
              "      <td>161.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047599</td>\n",
              "      <td>0.762776</td>\n",
              "      <td>0.002266</td>\n",
              "      <td>266.375000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.087122</td>\n",
              "      <td>0.047690</td>\n",
              "      <td>0.957743</td>\n",
              "      <td>0.002274</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48480</th>\n",
              "      <td>8.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059875</td>\n",
              "      <td>0.964159</td>\n",
              "      <td>0.003585</td>\n",
              "      <td>833.241667</td>\n",
              "      <td>14.308333</td>\n",
              "      <td>0.225464</td>\n",
              "      <td>0.058035</td>\n",
              "      <td>0.800377</td>\n",
              "      <td>0.003368</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48481</th>\n",
              "      <td>107.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047871</td>\n",
              "      <td>0.662947</td>\n",
              "      <td>0.002292</td>\n",
              "      <td>471.229167</td>\n",
              "      <td>13.287500</td>\n",
              "      <td>0.285032</td>\n",
              "      <td>0.149681</td>\n",
              "      <td>0.485837</td>\n",
              "      <td>0.022405</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48482</th>\n",
              "      <td>83.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096105</td>\n",
              "      <td>0.933519</td>\n",
              "      <td>0.009236</td>\n",
              "      <td>170.016667</td>\n",
              "      <td>5.783333</td>\n",
              "      <td>0.425507</td>\n",
              "      <td>0.125035</td>\n",
              "      <td>0.892728</td>\n",
              "      <td>0.015634</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48483</th>\n",
              "      <td>15.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050518</td>\n",
              "      <td>0.858753</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>269.358333</td>\n",
              "      <td>11.450000</td>\n",
              "      <td>0.140576</td>\n",
              "      <td>0.050346</td>\n",
              "      <td>0.927587</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48484</th>\n",
              "      <td>180.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061520</td>\n",
              "      <td>0.989119</td>\n",
              "      <td>0.003785</td>\n",
              "      <td>67.458333</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>0.239078</td>\n",
              "      <td>0.088878</td>\n",
              "      <td>0.962838</td>\n",
              "      <td>0.007899</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48485</th>\n",
              "      <td>66.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053115</td>\n",
              "      <td>0.745681</td>\n",
              "      <td>0.002821</td>\n",
              "      <td>304.941667</td>\n",
              "      <td>10.983333</td>\n",
              "      <td>0.157054</td>\n",
              "      <td>0.054566</td>\n",
              "      <td>0.845027</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48486</th>\n",
              "      <td>62.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>69.5</td>\n",
              "      <td>112.5</td>\n",
              "      <td>97.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049036</td>\n",
              "      <td>0.923443</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>385.316667</td>\n",
              "      <td>10.791667</td>\n",
              "      <td>0.175952</td>\n",
              "      <td>0.057810</td>\n",
              "      <td>0.905572</td>\n",
              "      <td>0.003342</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48487</th>\n",
              "      <td>172.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048502</td>\n",
              "      <td>0.739400</td>\n",
              "      <td>0.002352</td>\n",
              "      <td>1033.091667</td>\n",
              "      <td>22.450000</td>\n",
              "      <td>0.059882</td>\n",
              "      <td>0.048143</td>\n",
              "      <td>0.653258</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48488</th>\n",
              "      <td>240.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046491</td>\n",
              "      <td>0.210726</td>\n",
              "      <td>0.002161</td>\n",
              "      <td>1539.433333</td>\n",
              "      <td>22.025000</td>\n",
              "      <td>0.235616</td>\n",
              "      <td>0.074419</td>\n",
              "      <td>0.778272</td>\n",
              "      <td>0.005538</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48489</th>\n",
              "      <td>199.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.084214</td>\n",
              "      <td>0.843091</td>\n",
              "      <td>0.007092</td>\n",
              "      <td>1204.887500</td>\n",
              "      <td>19.062500</td>\n",
              "      <td>0.437706</td>\n",
              "      <td>0.236969</td>\n",
              "      <td>0.793580</td>\n",
              "      <td>0.056155</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48490</th>\n",
              "      <td>136.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047232</td>\n",
              "      <td>0.745728</td>\n",
              "      <td>0.002231</td>\n",
              "      <td>1903.945833</td>\n",
              "      <td>29.837500</td>\n",
              "      <td>0.054158</td>\n",
              "      <td>0.047140</td>\n",
              "      <td>0.775478</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48491</th>\n",
              "      <td>103.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>167.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050518</td>\n",
              "      <td>0.932066</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>189.229167</td>\n",
              "      <td>8.695833</td>\n",
              "      <td>0.128663</td>\n",
              "      <td>0.052291</td>\n",
              "      <td>0.958446</td>\n",
              "      <td>0.002734</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48492</th>\n",
              "      <td>120.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>231.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063601</td>\n",
              "      <td>0.968008</td>\n",
              "      <td>0.004045</td>\n",
              "      <td>124.625000</td>\n",
              "      <td>5.758333</td>\n",
              "      <td>0.306278</td>\n",
              "      <td>0.066012</td>\n",
              "      <td>0.938417</td>\n",
              "      <td>0.004358</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48493</th>\n",
              "      <td>92.5</td>\n",
              "      <td>196.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>216.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053522</td>\n",
              "      <td>0.958637</td>\n",
              "      <td>0.002865</td>\n",
              "      <td>110.929167</td>\n",
              "      <td>6.154167</td>\n",
              "      <td>0.278138</td>\n",
              "      <td>0.093773</td>\n",
              "      <td>0.979495</td>\n",
              "      <td>0.008793</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48494</th>\n",
              "      <td>65.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>202.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048233</td>\n",
              "      <td>0.845225</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>278.500000</td>\n",
              "      <td>9.275000</td>\n",
              "      <td>0.165416</td>\n",
              "      <td>0.054247</td>\n",
              "      <td>0.914065</td>\n",
              "      <td>0.002943</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48495</th>\n",
              "      <td>77.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050260</td>\n",
              "      <td>0.729970</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>1365.429167</td>\n",
              "      <td>24.112500</td>\n",
              "      <td>0.056649</td>\n",
              "      <td>0.047781</td>\n",
              "      <td>0.813291</td>\n",
              "      <td>0.002283</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48496</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049913</td>\n",
              "      <td>0.778716</td>\n",
              "      <td>0.002491</td>\n",
              "      <td>399.979167</td>\n",
              "      <td>14.279167</td>\n",
              "      <td>0.112148</td>\n",
              "      <td>0.051201</td>\n",
              "      <td>0.887823</td>\n",
              "      <td>0.002622</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48497</th>\n",
              "      <td>138.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055590</td>\n",
              "      <td>0.830110</td>\n",
              "      <td>0.003090</td>\n",
              "      <td>1618.029167</td>\n",
              "      <td>25.437500</td>\n",
              "      <td>0.116033</td>\n",
              "      <td>0.054327</td>\n",
              "      <td>0.773517</td>\n",
              "      <td>0.002951</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48498</th>\n",
              "      <td>173.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.148312</td>\n",
              "      <td>0.929718</td>\n",
              "      <td>0.021997</td>\n",
              "      <td>541.945833</td>\n",
              "      <td>16.862500</td>\n",
              "      <td>0.113708</td>\n",
              "      <td>0.053926</td>\n",
              "      <td>0.895961</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48499</th>\n",
              "      <td>173.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050173</td>\n",
              "      <td>0.761865</td>\n",
              "      <td>0.002517</td>\n",
              "      <td>188.966667</td>\n",
              "      <td>7.841667</td>\n",
              "      <td>0.205626</td>\n",
              "      <td>0.063670</td>\n",
              "      <td>0.768204</td>\n",
              "      <td>0.004054</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48498 rows × 369 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       raw_0  raw_1  raw_2  raw_3  raw_4  raw_5  raw_6  raw_7  raw_8  raw_9  \\\n",
              "2       45.0  239.0   66.0  252.0  140.0   63.0  249.0  250.0   33.0  248.0   \n",
              "3      126.0  239.0  137.0  243.5  120.0  128.0  102.0  225.0  125.0  238.0   \n",
              "4      226.0  222.0  153.0  235.0  171.0  148.0  231.0  231.0  225.0  229.0   \n",
              "5      169.0   81.0  100.0   44.0  104.0   93.0   35.0   65.0  170.0   89.0   \n",
              "6      154.0  145.0   76.0  134.0  175.0   53.0  134.0  141.0  147.0  148.0   \n",
              "7       77.0   35.0  105.0  108.0  112.0  107.0  173.0   79.0   88.0   53.0   \n",
              "8      104.0   55.0  113.0   90.0  107.0  124.0   79.0   88.0  115.0   72.0   \n",
              "9       97.0  253.0  255.0  251.0  141.0  255.0  248.0  252.0   51.0  255.0   \n",
              "10     224.0  227.0   84.0  214.0   57.0   37.0  215.0  218.0  207.0  214.0   \n",
              "11     153.0   13.0   51.0   50.0   58.0   48.0   21.0   13.0   21.0   12.0   \n",
              "12      82.0  213.0   61.0   98.0   59.0   67.0   74.0   71.0   91.0   61.0   \n",
              "13     105.5   40.0   84.0    7.0   39.0   91.0   16.0   18.0   29.0   19.0   \n",
              "14     129.0  103.0   65.0  159.0   75.0   56.0  152.0  156.0  114.0  145.0   \n",
              "15     123.0  128.0  116.0  198.0  111.0  131.0  148.0  186.0  128.0  185.0   \n",
              "16      38.0   62.0   95.5   83.0   39.0   36.0  156.0  119.0   54.0  135.0   \n",
              "17     106.0   17.0   75.0  119.5   91.0   92.0  164.0  174.0  102.0  173.0   \n",
              "18     169.0  186.0  158.0  156.0  152.0  143.0  181.0  176.0  150.0  180.0   \n",
              "19     179.0  179.0  127.0  130.0  115.0  107.0  205.0  196.0  185.0  189.0   \n",
              "20      58.0  139.0   54.0  194.0   78.0   69.0  199.0  205.0   49.0  198.0   \n",
              "21      88.0   83.0  135.0  109.0   53.0  109.0   79.0   82.0   70.0   87.0   \n",
              "22      52.0   51.0  112.0  199.0  114.0  111.0   98.0  183.0   96.0  137.0   \n",
              "23     110.5   95.0  145.0   44.0  156.0  155.0   53.0  144.0   80.0   57.0   \n",
              "24     169.0   90.0  178.0  251.0  157.0  173.0  126.0  105.0  206.0  104.0   \n",
              "25     108.0  254.0   84.0  249.0   96.0   85.0  247.0  253.0  198.5  254.0   \n",
              "26     189.0  182.0  136.0  224.0  153.0  136.0  255.0  226.0  191.0  210.0   \n",
              "27       3.0    3.0   97.0    6.0  173.5  104.0    7.0    8.0    4.0    8.0   \n",
              "28     168.0  175.0  128.0  157.0  194.0  149.0  156.0  161.0  165.0  154.0   \n",
              "29      64.0   41.0   89.0   66.0   69.0   89.0   76.0   57.0   38.0   49.0   \n",
              "30     250.0  143.0  108.0  109.0   73.0   32.0  248.0  120.5  246.0  130.0   \n",
              "31     167.0   74.0  162.0  147.0  122.0  193.0  211.0  184.0  174.0  183.0   \n",
              "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "48470   90.0   45.0  127.0   35.0  126.0  124.0   56.0   46.0   77.0   39.0   \n",
              "48471   76.0   34.0  133.0   32.0  136.0  133.0   41.0   40.0   67.0   53.0   \n",
              "48472   45.0   92.0  134.0  108.0  106.0  109.0  107.0  110.0   43.0   90.0   \n",
              "48473  183.0  160.0  196.0  166.0  148.0  181.0  163.0  161.0  127.5  166.0   \n",
              "48474  254.0  250.0   58.0   56.0   85.0   54.0  142.0  105.0  212.0  172.0   \n",
              "48475  164.0   26.0   60.0  129.0  102.0  101.0  121.0   64.0  169.0   81.0   \n",
              "48476  128.0  113.0  147.0  110.0  116.0   74.5  100.0   86.5  145.0   99.0   \n",
              "48477   84.0   90.0   85.0  102.0   41.0   48.0  114.0  109.0  124.5  113.0   \n",
              "48478  109.0  169.0  128.0  142.0   80.0  113.0  129.5  158.0  104.0  142.0   \n",
              "48479  155.0  100.0   68.0  120.0  118.0   59.0  145.0  167.5  161.0  152.0   \n",
              "48480    8.0  132.0   60.0  188.0   63.0   58.0  179.0  177.0    1.0  170.0   \n",
              "48481  107.0   99.0  156.0  101.0  118.0  114.0  119.0  127.0  108.0  105.0   \n",
              "48482   83.0   59.0   54.0   63.0   58.0   58.0   59.0   63.0  119.0  115.0   \n",
              "48483   15.0   93.0   92.0   81.0   83.0   80.0  113.0  119.0   15.0  125.0   \n",
              "48484  180.0  140.0   27.0  131.0   34.0   33.0  140.0  134.0  175.0  132.0   \n",
              "48485   66.0   47.0   72.0   68.0   70.0   93.0   39.0   39.0   58.0   35.0   \n",
              "48486   62.0   85.0  151.0   96.0  114.0  158.0   92.0   69.5  112.5   97.0   \n",
              "48487  172.0   98.0  130.0  130.0  158.0  123.0  103.0  100.0  167.0  104.0   \n",
              "48488  240.0  172.0    5.0  163.0    9.0    7.0  164.0  152.0  239.0  151.0   \n",
              "48489  199.0  182.0  255.0  255.0  252.0  255.0  253.0  255.0  209.0  253.0   \n",
              "48490  136.0  106.0   47.0  107.0   38.0   59.0   78.0  105.0   79.0  104.0   \n",
              "48491  103.0   52.0   53.0   35.0   38.0   56.0   19.0   23.0  137.0  167.5   \n",
              "48492  120.0  224.0   84.0  221.0   76.0   82.0  218.0  228.0  104.0  231.0   \n",
              "48493   92.5  196.0   73.0  107.0   11.0   25.0   76.0   71.0   24.0  216.5   \n",
              "48494   65.0  139.0   34.0  242.0   47.0   37.0  193.0  205.0  108.0  202.0   \n",
              "48495   77.0   85.0   75.0   75.0  122.0   86.0  129.0  117.0   67.0  155.0   \n",
              "48496    1.0    0.0  131.0  108.0  132.0  128.0   45.0   72.0   11.0   90.0   \n",
              "48497  138.0   96.0  140.0  252.0  147.0  136.0  147.0  232.0   96.0  224.0   \n",
              "48498  173.0  255.0   91.0  254.0  127.0   78.0  225.0  255.0  197.0  253.0   \n",
              "48499  173.0  192.0  168.0  125.0  177.0  168.0  118.0  126.0  136.0  122.0   \n",
              "\n",
              "      ...    GLCM_15   GLCM_16   GLCM_17      GLCM_18    GLCM_19   GLCM_20  \\\n",
              "2     ...   0.047048  0.790219  0.002214   319.633333  12.100000  0.088411   \n",
              "3     ...   0.050260  0.851072  0.002526   236.862500  11.520833  0.094054   \n",
              "4     ...   0.050346  0.875646  0.002535   720.466667  16.183333  0.133303   \n",
              "5     ...   0.047048  0.711023  0.002214   433.141667  13.550000  0.101400   \n",
              "6     ...   0.050260  0.852472  0.002526   325.300000  12.533333  0.122460   \n",
              "7     ...   0.047690  0.856955  0.002274  1002.316667  23.733333  0.058546   \n",
              "8     ...   0.053359  0.888191  0.002847   185.091667   9.266667  0.112188   \n",
              "9     ...   0.048143  0.853129  0.002318  1013.541667  17.600000  0.226261   \n",
              "10    ...   0.048052  0.869933  0.002309  1220.070833  21.679167  0.114511   \n",
              "11    ...   0.056596  0.927067  0.003203  1356.254167  19.379167  0.174106   \n",
              "12    ...   0.051201  0.837024  0.002622  1488.866667  22.566667  0.107707   \n",
              "13    ...   0.053765  0.929167  0.002891   637.754167  13.487500  0.184824   \n",
              "14    ...   0.048143  0.829038  0.002318   227.700000  11.708333  0.082822   \n",
              "15    ...   0.064077  0.851049  0.004106   125.158333   7.383333  0.208491   \n",
              "16    ...   0.049036  0.889924  0.002405   481.875000  12.666667  0.152741   \n",
              "17    ...   0.049564  0.737556  0.002457   383.187500  13.445833  0.102674   \n",
              "18    ...   0.047690  0.846624  0.002274  1476.270833  25.670833  0.072999   \n",
              "19    ...   0.060524  0.869558  0.003663   362.629167  12.887500  0.103488   \n",
              "20    ...   0.050604  0.879202  0.002561   244.470833  11.704167  0.111373   \n",
              "21    ...   0.047416  0.719149  0.002248   563.062500  17.537500  0.087897   \n",
              "22    ...   0.060668  0.952798  0.003681   318.991667  10.150000  0.212709   \n",
              "23    ...   0.056826  0.781407  0.003229   474.608333  12.150000  0.166958   \n",
              "24    ...   0.052540  0.790246  0.002760   769.670833  17.354167  0.096294   \n",
              "25    ...   0.048681  0.625437  0.002370   346.966667  12.508333  0.105584   \n",
              "26    ...   0.052457  0.829801  0.002752   162.579167   9.429167  0.108925   \n",
              "27    ...   0.053926  0.890963  0.002908   468.575000  14.166667  0.114212   \n",
              "28    ...   0.049739  0.753276  0.002474  1152.820833  22.995833  0.064679   \n",
              "29    ...   0.055979  0.902675  0.003134   470.454167  14.362500  0.123147   \n",
              "30    ...   0.047599  0.865270  0.002266  1751.454167  28.370833  0.073667   \n",
              "31    ...   0.053522  0.958077  0.002865   331.066667  12.216667  0.098480   \n",
              "...   ...        ...       ...       ...          ...        ...       ...   \n",
              "48470 ...   0.049564  0.727174  0.002457   157.241667   8.158333  0.184644   \n",
              "48471 ...   0.050518  0.754752  0.002552   186.733333   8.400000  0.151110   \n",
              "48472 ...   0.047048  0.782937  0.002214   496.625000  15.450000  0.061258   \n",
              "48473 ...   0.056519  0.881859  0.003194   581.775000  16.833333  0.117120   \n",
              "48474 ...   0.047781  0.753503  0.002283  1447.687500  23.220833  0.097062   \n",
              "48475 ...   0.048143  0.838396  0.002318   555.604167  18.129167  0.066879   \n",
              "48476 ...   0.049301  0.900144  0.002431   390.979167  14.654167  0.089177   \n",
              "48477 ...   0.056596  0.709470  0.003203   643.887500  17.937500  0.099861   \n",
              "48478 ...   0.050346  0.873416  0.002535    95.345833   6.462500  0.189998   \n",
              "48479 ...   0.047599  0.762776  0.002266   266.375000  12.000000  0.087122   \n",
              "48480 ...   0.059875  0.964159  0.003585   833.241667  14.308333  0.225464   \n",
              "48481 ...   0.047871  0.662947  0.002292   471.229167  13.287500  0.285032   \n",
              "48482 ...   0.096105  0.933519  0.009236   170.016667   5.783333  0.425507   \n",
              "48483 ...   0.050518  0.858753  0.002552   269.358333  11.450000  0.140576   \n",
              "48484 ...   0.061520  0.989119  0.003785    67.458333   5.100000  0.239078   \n",
              "48485 ...   0.053115  0.745681  0.002821   304.941667  10.983333  0.157054   \n",
              "48486 ...   0.049036  0.923443  0.002405   385.316667  10.791667  0.175952   \n",
              "48487 ...   0.048502  0.739400  0.002352  1033.091667  22.450000  0.059882   \n",
              "48488 ...   0.046491  0.210726  0.002161  1539.433333  22.025000  0.235616   \n",
              "48489 ...   0.084214  0.843091  0.007092  1204.887500  19.062500  0.437706   \n",
              "48490 ...   0.047232  0.745728  0.002231  1903.945833  29.837500  0.054158   \n",
              "48491 ...   0.050518  0.932066  0.002552   189.229167   8.695833  0.128663   \n",
              "48492 ...   0.063601  0.968008  0.004045   124.625000   5.758333  0.306278   \n",
              "48493 ...   0.053522  0.958637  0.002865   110.929167   6.154167  0.278138   \n",
              "48494 ...   0.048233  0.845225  0.002326   278.500000   9.275000  0.165416   \n",
              "48495 ...   0.050260  0.729970  0.002526  1365.429167  24.112500  0.056649   \n",
              "48496 ...   0.049913  0.778716  0.002491   399.979167  14.279167  0.112148   \n",
              "48497 ...   0.055590  0.830110  0.003090  1618.029167  25.437500  0.116033   \n",
              "48498 ...   0.148312  0.929718  0.021997   541.945833  16.862500  0.113708   \n",
              "48499 ...   0.050173  0.761865  0.002517   188.966667   7.841667  0.205626   \n",
              "\n",
              "        GLCM_21   GLCM_22   GLCM_23   xg  \n",
              "2      0.049826  0.855383  0.002483  9.0  \n",
              "3      0.050346  0.789885  0.002535  7.0  \n",
              "4      0.051539  0.890142  0.002656  8.0  \n",
              "5      0.049036  0.902981  0.002405  4.0  \n",
              "6      0.050173  0.910305  0.002517  1.0  \n",
              "7      0.047871  0.844166  0.002292  5.0  \n",
              "8      0.049826  0.901720  0.002483  4.0  \n",
              "9      0.092327  0.924492  0.008524  9.0  \n",
              "10     0.048858  0.798455  0.002387  9.0  \n",
              "11     0.058704  0.869596  0.003446  5.0  \n",
              "12     0.056519  0.694368  0.003194  3.0  \n",
              "13     0.054566  0.873465  0.002977  1.0  \n",
              "14     0.048233  0.917249  0.002326  4.0  \n",
              "15     0.059293  0.878872  0.003516  0.0  \n",
              "16     0.055979  0.852423  0.003134  6.0  \n",
              "17     0.050775  0.775500  0.002578  5.0  \n",
              "18     0.050861  0.612740  0.002587  1.0  \n",
              "19     0.049213  0.908461  0.002422  0.0  \n",
              "20     0.051539  0.612850  0.002656  4.0  \n",
              "21     0.048947  0.901529  0.002396  9.0  \n",
              "22     0.063942  0.857735  0.004089  1.0  \n",
              "23     0.057130  0.838909  0.003264  7.0  \n",
              "24     0.049036  0.779746  0.002405  9.0  \n",
              "25     0.050946  0.705397  0.002595  7.0  \n",
              "26     0.051370  0.876130  0.002639  5.0  \n",
              "27     0.050604  0.878158  0.002561  2.0  \n",
              "28     0.047048  0.692666  0.002214  6.0  \n",
              "29     0.050604  0.799103  0.002561  3.0  \n",
              "30     0.048233  0.815526  0.002326  5.0  \n",
              "31     0.050260  0.915835  0.002526  3.0  \n",
              "...         ...       ...       ...  ...  \n",
              "48470  0.057509  0.786274  0.003307  7.0  \n",
              "48471  0.056443  0.800287  0.003186  7.0  \n",
              "48472  0.048858  0.753986  0.002387  5.0  \n",
              "48473  0.051116  0.819848  0.002613  0.0  \n",
              "48474  0.049476  0.799342  0.002448  9.0  \n",
              "48475  0.047507  0.748635  0.002257  3.0  \n",
              "48476  0.048143  0.911876  0.002318  3.0  \n",
              "48477  0.049301  0.589718  0.002431  7.0  \n",
              "48478  0.057206  0.968339  0.003273  4.0  \n",
              "48479  0.047690  0.957743  0.002274  3.0  \n",
              "48480  0.058035  0.800377  0.003368  9.0  \n",
              "48481  0.149681  0.485837  0.022405  7.0  \n",
              "48482  0.125035  0.892728  0.015634  0.0  \n",
              "48483  0.050346  0.927587  0.002535  3.0  \n",
              "48484  0.088878  0.962838  0.007899  2.0  \n",
              "48485  0.054566  0.845027  0.002977  6.0  \n",
              "48486  0.057810  0.905572  0.003342  1.0  \n",
              "48487  0.048143  0.653258  0.002318  6.0  \n",
              "48488  0.074419  0.778272  0.005538  3.0  \n",
              "48489  0.236969  0.793580  0.056155  0.0  \n",
              "48490  0.047140  0.775478  0.002222  7.0  \n",
              "48491  0.052291  0.958446  0.002734  8.0  \n",
              "48492  0.066012  0.938417  0.004358  8.0  \n",
              "48493  0.093773  0.979495  0.008793  3.0  \n",
              "48494  0.054247  0.914065  0.002943  8.0  \n",
              "48495  0.047781  0.813291  0.002283  4.0  \n",
              "48496  0.051201  0.887823  0.002622  4.0  \n",
              "48497  0.054327  0.773517  0.002951  7.0  \n",
              "48498  0.053926  0.895961  0.002908  3.0  \n",
              "48499  0.063670  0.768204  0.004054  7.0  \n",
              "\n",
              "[48498 rows x 369 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEvkfjyzqBkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(ndf,labels,test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLYNXKdKp_cB",
        "colab_type": "code",
        "outputId": "8bd204f9-dde2-4f30-e1cb-7c094a639226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest=RandomForestClassifier(max_depth=8,n_estimators=50)\n",
        "forest.fit(X_train,Y_train)\n",
        "ypred6=forest.predict(X_test)\n",
        "print (sklearn.metrics.accuracy_score(Y_test.values,ypred6))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.547766323024055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ0k8yFNqwWI",
        "colab_type": "code",
        "outputId": "3109d00f-25d9-46f8-fd54-9bd1faa5c426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "et=ExtraTreesClassifier(max_depth=15,n_estimators=200)\n",
        "et.fit(X_train,Y_train)\n",
        "ypred0=et.predict(X_test)\n",
        "print (sklearn.metrics.accuracy_score(Y_test.values,ypred0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6692096219931272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxKDjs_Crrvm",
        "colab_type": "code",
        "outputId": "d3375bc3-8282-488d-c0a3-993f7c33447f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ada=AdaBoostClassifier(n_estimators=50,learning_rate=0.5)\n",
        "ada.fit(X_train,Y_train)\n",
        "ypred8=ada.predict(X_test)\n",
        "print (sklearn.metrics.accuracy_score(Y_test.values,ypred8))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7678350515463918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywAxL5uEsDwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tst=test.iloc[:,0:368].interpolate()\n",
        "tst['xg']=xgbcl.predict(tst)\n",
        "# tst.isna().sum()\n",
        "arr=ada.predict(tst)\n",
        "# arr.DataFrame('bagging.csv',index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi94KPe2s2OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr=arr.astype(int)\n",
        "pd.DataFrame(arr).to_csv('bagging.csv',index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfLDNq5bQsGJ",
        "colab_type": "code",
        "outputId": "7339ff4d-e12f-4386-aafd-2a267976e23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "parameters = {\n",
        "    \"learning_rate\": [0.3, 0.5],\n",
        "    \"max_depth\":[3,5,7],\n",
        "    \"n_estimators\":[100,150]\n",
        "    }\n",
        "clf = GridSearchCV(XGBClassifier(), parameters, cv=2, n_jobs=-1,verbose=8)\n",
        "clf.fit(X_train, Y_train)\n",
        "print(clf.score(X_train, Y_train))\n",
        "print(clf.best_params_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  9.0min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dAAKaJAb5Jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5,max_depth=3, random_state=0)\n",
        "clf.fit(X_train, Y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print (sklearn.metrics.accuracy_score(Y_test.values,pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5WythhcwCTS",
        "colab_type": "text"
      },
      "source": [
        "# Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq_WPdnBUKUj",
        "colab_type": "code",
        "outputId": "03714942-8238-4d03-99df-4ee294772618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!pip install mlens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlens\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/f7/c04bda423ac93ddb54bc4c3a21c79c9a24bc83844efc30dc4c11c289e894/mlens-0.2.3-py2.py3-none-any.whl (227kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python2.7/dist-packages (from mlens) (1.16.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python2.7/dist-packages (from mlens) (1.2.1)\n",
            "Installing collected packages: mlens\n",
            "Successfully installed mlens-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAlRATbz22My",
        "colab_type": "code",
        "outputId": "0e72b454-b1b9-4469-f70b-3966e158d297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlens.ensemble import SuperLearner \n",
        "from sklearn.metrics import f1_score\n",
        "import sklearn\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MLENS] backend: threading\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpXBGrtjUS_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('./DataSet/Train.csv')\n",
        "test=pd.read_csv('./DataSet/Test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wwj7PsqwArm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_train=train.copy()\n",
        "normalized_train=normalized_train.drop(normalized_train.iloc[:,368:1392],axis=1)\n",
        "normalized_train=normalized_train.interpolate()\n",
        "normalized_train=normalized_train.dropna()\n",
        "labels=normalized_train.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf_TgaXQ0QqX",
        "colab_type": "code",
        "outputId": "ba2af1e4-ec61-4c99-ebd2-b958cdf689a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ndf=normalized_train.iloc[:,0:368].copy()\n",
        "ndf = np.array(ndf)\n",
        "ndfs = np.square(ndf)\n",
        "ndfc = np.power(ndf,3)\n",
        "ndf = np.hstack((ndf,ndfs,ndfc))\n",
        "print ndf.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48498, 1104)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9q3EMLGZxEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#standardizing the input feature\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "ndf = sc.fit_transform(ndf)\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(ndf,labels,test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpn49zZcaH86",
        "colab_type": "code",
        "outputId": "5db0e5f1-08fd-46c7-fff5-13e3d0091413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# y_train = np_utils.to_categorical(Y_train, 10)\n",
        "# y_test = np_utils.to_categorical(Y_test, 10)\n",
        "from keras.callbacks import EarlyStopping\n",
        "n_cols = X_train.shape[1]\n",
        "#set early stopping monitor so the model stops training when it won't improve anymore\n",
        "early_stopping_monitor = EarlyStopping(patience=3)\n",
        "model_mc = Sequential()\n",
        "#add model layers\n",
        "model_mc.add(Dense(200, activation='relu', input_shape=(n_cols,)))\n",
        "model_mc.add(Dense(200, activation='relu'))\n",
        "model_mc.add(Dense(200, activation='relu'))\n",
        "model_mc.add(Dense(10))\n",
        "\n",
        "#compile model using mse as a measure of model performance\n",
        "model_mc.compile(optimizer='adam', loss='mean_squared_error')\n",
        "#train model\n",
        "model_mc.fit(X_train, y_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 27158 samples, validate on 6790 samples\n",
            "Epoch 1/30\n",
            "27158/27158 [==============================] - 11s 393us/step - loss: 0.0837 - val_loss: 0.0716\n",
            "Epoch 2/30\n",
            "27158/27158 [==============================] - 9s 346us/step - loss: 0.0673 - val_loss: 0.0688\n",
            "Epoch 3/30\n",
            "27158/27158 [==============================] - 10s 366us/step - loss: 0.0620 - val_loss: 0.0671\n",
            "Epoch 4/30\n",
            "27158/27158 [==============================] - 10s 381us/step - loss: 0.0581 - val_loss: 0.0665\n",
            "Epoch 5/30\n",
            "27158/27158 [==============================] - 10s 372us/step - loss: 0.0543 - val_loss: 0.0659\n",
            "Epoch 6/30\n",
            "27158/27158 [==============================] - 10s 360us/step - loss: 0.0510 - val_loss: 0.0660\n",
            "Epoch 7/30\n",
            "27158/27158 [==============================] - 10s 352us/step - loss: 0.0475 - val_loss: 0.0676\n",
            "Epoch 8/30\n",
            "27158/27158 [==============================] - 10s 368us/step - loss: 0.0442 - val_loss: 0.0683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4f3a59c110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGlxZJvgcDer",
        "colab_type": "code",
        "outputId": "37e57cb1-e1d1-490c-c215-5b0cd8d78181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred = model_mc.predict_classes(X_test)\n",
        "print np.sum(pred==Y_test)/float(len(Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49876288659793816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcWgCFsE2Ghz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgbcl = XGBClassifier(n_estimators=200,max_depth=4,verbosity=3,learning_rate=0.3)\n",
        "xgbcl.fit(X_train, Y_train)\n",
        "pred=xgbcl.predict(X_test)\n",
        "print sklearn.metrics.accuracy_score(Y_test.values,pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3hRB78N2EvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ndf=normalized_train.iloc[:,0:368].copy()\n",
        "# ndf['pixel']=ndf.iloc[:,0:200].mean(axis=1)\n",
        "# ndf['gmean']=ndf.iloc[:,344:368].mean(axis=1)\n",
        "# ndf=ndf.drop(ndf.iloc[:,344:368],axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHJ5cWSYwkrt",
        "colab_type": "code",
        "outputId": "0221f389-2d3b-4d87-802c-37dc3428c080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "ndf.iloc[:,:368].columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([u'raw_0', u'raw_1', u'raw_2', u'raw_3', u'raw_4', u'raw_5', u'raw_6',\n",
              "       u'raw_7', u'raw_8', u'raw_9',\n",
              "       ...\n",
              "       u'hog_136', u'hog_137', u'hog_138', u'hog_139', u'hog_140', u'hog_141',\n",
              "       u'hog_142', u'hog_143', u'pixel', u'gmean'],\n",
              "      dtype='object', length=346)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4ipd3CwwkqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICpamaeg2Ztv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ada=AdaBoostClassifier(n_estimators=100,learning_rate=0.5)\n",
        "et=ExtraTreesClassifier(max_depth=8,n_estimators=50)\n",
        "rf=RandomForestClassifier(max_depth=8,n_estimators=50)\n",
        "\n",
        "xgbcl = XGBClassifier(n_estimators=100,max_depth=3,n_jobs=-1,learning_rate=0.5)\n",
        "et2=ExtraTreesClassifier(max_depth=5,n_estimators=50)\n",
        "\n",
        "\n",
        "\n",
        "lr = LogisticRegression(solver='lbfgs',multi_class='auto',max_iter=250)\n",
        "\n",
        "base_learners = [\n",
        "    ('et', et), ('rf', rf), ('xg', xgbcl)\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "base_learner2 = [\n",
        "    ('lr',lr), ('ada', ada)\n",
        "]\n",
        "\n",
        "meta_learner=XGBClassifier(n_estimators=50,max_depth=5,n_jobs=-1,learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLwDmDE13leO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(ndf,labels,test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ez3nZe3lc7",
        "colab_type": "code",
        "outputId": "4d37d487-c221-4bee-9e16-d636578e0194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# features=[]\n",
        "# for i in range(344,368,1):\n",
        "#   features.append(i)\n",
        "# print features \n",
        "print X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33948, 736)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyYfjbttwBo5",
        "colab_type": "code",
        "outputId": "0335c8c6-5e4a-4253-f6ca-c81597a8bd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1526
        }
      },
      "source": [
        "ensemble = SuperLearner(scorer=sklearn.metrics.accuracy_score,verbose=3)\n",
        "\n",
        "# First layer\n",
        "ensemble.add(base_learners)\n",
        "ensemble.add(base_learner2)\n",
        "\n",
        "# Final meta estimator\n",
        "ensemble.add_meta(meta_learner)\n",
        "# Train ensemble\n",
        "ensemble.fit(X_train, Y_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting 3 layers\n",
            "Processing layer-1            \n",
            "Learners ...                   et.0.1                         done | 00:00:03\n",
            "et.0.0                         done | 00:00:06\n",
            "et.0.2                         done | 00:00:03\n",
            "rf.0.1                         done | 00:00:11\n",
            "rf.0.0                         done | 00:00:22\n",
            "rf.0.2                         done | 00:00:11\n",
            "xg.0.1                         done | 00:06:30\n",
            "xg.0.0                         done | 00:12:45\n",
            "xg.0.2                         done | 00:06:23done | 00:13:23\n",
            "\n",
            "layer-1                        done | 00:13:23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-406c012b6763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_learner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mlens/ensemble/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mlens/ensemble/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         with ParallelProcessing(self.backend, self.n_jobs,\n\u001b[1;32m    157\u001b[0m                                 max(self.verbose - 4, 0)) as manager:\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mlens/parallel/backend.pyc\u001b[0m in \u001b[0;36mstack\u001b[0;34m(self, caller, job, X, y, path, return_preds, warm_start, split, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mjob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             return_preds=return_preds, split=split, stack=True)\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mlens/parallel/backend.pyc\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, caller, out, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partial_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturn_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mlens/parallel/backend.pyc\u001b[0m in \u001b[0;36m_partial_process\u001b[0;34m(self, task, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__no_output__\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_feature_prop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_propagate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mlens/parallel/backend.pyc\u001b[0m in \u001b[0;36m_propagate_features\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;31m# Simple item setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mp_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_feature_prop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# Need to populate propagated features using scipy sparse hstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9s44Qwe80Ks",
        "colab_type": "code",
        "outputId": "2e86146a-4cd6-4c41-feb5-cb84d4256e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "pred=ensemble.predict(X_test)\n",
        "print (sklearn.metrics.accuracy_score(Y_test.values,pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Predicting 3 layers\n",
            "Processing layer-1            \n",
            "Learners ...                   rf.0.0                         done | 00:00:00\n",
            "et.0.0                         done | 00:00:00\n",
            "xg.0.0                         done | 00:00:00\n",
            "done | 00:00:01\n",
            "layer-1                        done | 00:00:01\n",
            "Processing layer-2            \n",
            "Learners ...                   lr.0.0                         done | 00:00:00\n",
            "ada.0.0                        done | 00:00:00\n",
            "done | 00:00:00\n",
            "layer-2                        done | 00:00:00\n",
            "Processing layer-3            \n",
            "Learners ...                   xgbclassifier.0.0              done | 00:00:00\n",
            "done | 00:00:00\n",
            "layer-3                        done | 00:00:00\n",
            "Predict complete                    | 00:00:02\n",
            "0.5168384879725086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhT7MGG6dAed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of hard voting \n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "clf = [rf, et, knn, svc, rg]\n",
        "eclf = VotingClassifier(estimators=[('Random Forests', rf), ('Extra Trees', et), ('SVC', svc), ('Ridge Classifier', rg)], voting='hard')\n",
        "for clf, label in zip([rf, et, knn, svc, rg, eclf], ['Random Forest', 'Extra Trees', 'SVC', 'Ridge Classifier', 'Ensemble']):\n",
        "    scores = cross_val_score(clf, X_train, Y_train, cv=10, scoring='accuracy')\n",
        "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lF3KWIRLHwg",
        "colab_type": "code",
        "outputId": "ee05ac93-5ea7-420a-f38d-dd7e8bbbc758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgbcl = XGBClassifier(n_estimators=50,max_depth=4,verbosity=True,n_jobs=-1,learning_rate=0.5)\n",
        "xgbcl.fit(X_train.iloc[:,0:144], Y_train)\n",
        "pred=xgbcl.predict(X_test.iloc[:,0:144])\n",
        "print sklearn.metrics.accuracy_score(Y_test.values,pred)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4149828178694158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5JASzf2LuD7",
        "colab_type": "code",
        "outputId": "a6089e5f-7ad0-40e5-e091-4804c4947e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 28288
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=500,verbose=1,shuffle=True)\n",
        "clf.fit(X_train, Y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print sklearn.metrics.accuracy_score(Y_test.values,pred)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Norm: 9341.92, NNZs: 285, Bias: -23.103494, T: 15174756, Avg. loss: 164.320271\n",
            "Total training time: 46.74 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 9342.06, NNZs: 291, Bias: -23.145249, T: 15208704, Avg. loss: 165.091830\n",
            "Total training time: 46.85 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 9342.21, NNZs: 283, Bias: -23.186817, T: 15242652, Avg. loss: 163.430314\n",
            "Total training time: 46.96 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 9342.35, NNZs: 286, Bias: -23.214127, T: 15276600, Avg. loss: 164.778310\n",
            "Total training time: 47.07 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 9342.49, NNZs: 291, Bias: -23.246118, T: 15310548, Avg. loss: 166.093764\n",
            "Total training time: 47.18 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 9342.62, NNZs: 294, Bias: -23.274253, T: 15344496, Avg. loss: 170.404320\n",
            "Total training time: 47.29 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 9342.75, NNZs: 285, Bias: -23.313425, T: 15378444, Avg. loss: 167.507868\n",
            "Total training time: 47.40 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 9342.90, NNZs: 282, Bias: -23.335763, T: 15412392, Avg. loss: 160.887408\n",
            "Total training time: 47.51 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 9343.05, NNZs: 280, Bias: -23.364038, T: 15446340, Avg. loss: 157.330601\n",
            "Total training time: 47.62 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 9343.20, NNZs: 281, Bias: -23.400317, T: 15480288, Avg. loss: 161.089276\n",
            "Total training time: 47.73 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 9343.34, NNZs: 292, Bias: -23.438753, T: 15514236, Avg. loss: 161.463992\n",
            "Total training time: 47.84 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 9343.48, NNZs: 283, Bias: -23.481893, T: 15548184, Avg. loss: 162.134946\n",
            "Total training time: 47.95 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 9343.61, NNZs: 270, Bias: -23.515294, T: 15582132, Avg. loss: 162.598691\n",
            "Total training time: 48.06 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 9343.76, NNZs: 283, Bias: -23.549949, T: 15616080, Avg. loss: 154.943875\n",
            "Total training time: 48.16 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 9343.90, NNZs: 271, Bias: -23.584101, T: 15650028, Avg. loss: 158.156911\n",
            "Total training time: 48.29 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 9344.05, NNZs: 282, Bias: -23.613407, T: 15683976, Avg. loss: 156.071100\n",
            "Total training time: 48.40 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 9344.19, NNZs: 279, Bias: -23.651477, T: 15717924, Avg. loss: 157.871865\n",
            "Total training time: 48.51 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 9344.32, NNZs: 284, Bias: -23.678558, T: 15751872, Avg. loss: 160.924037\n",
            "Total training time: 48.62 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 9344.45, NNZs: 293, Bias: -23.716826, T: 15785820, Avg. loss: 160.981978\n",
            "Total training time: 48.73 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 9344.57, NNZs: 291, Bias: -23.753471, T: 15819768, Avg. loss: 162.177453\n",
            "Total training time: 48.84 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 9344.70, NNZs: 286, Bias: -23.791471, T: 15853716, Avg. loss: 160.267533\n",
            "Total training time: 48.96 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 9344.83, NNZs: 289, Bias: -23.826568, T: 15887664, Avg. loss: 157.082315\n",
            "Total training time: 49.07 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 9344.96, NNZs: 289, Bias: -23.865018, T: 15921612, Avg. loss: 160.747737\n",
            "Total training time: 49.18 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 9345.09, NNZs: 285, Bias: -23.898335, T: 15955560, Avg. loss: 155.901208\n",
            "Total training time: 49.29 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 9345.22, NNZs: 293, Bias: -23.933214, T: 15989508, Avg. loss: 158.265001\n",
            "Total training time: 49.40 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 9345.35, NNZs: 286, Bias: -23.972522, T: 16023456, Avg. loss: 160.719255\n",
            "Total training time: 49.51 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 9345.47, NNZs: 282, Bias: -24.006880, T: 16057404, Avg. loss: 156.464916\n",
            "Total training time: 49.63 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 9345.61, NNZs: 280, Bias: -24.035481, T: 16091352, Avg. loss: 153.539560\n",
            "Total training time: 49.74 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 9345.75, NNZs: 285, Bias: -24.064555, T: 16125300, Avg. loss: 153.291357\n",
            "Total training time: 49.85 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 9345.87, NNZs: 285, Bias: -24.097322, T: 16159248, Avg. loss: 150.263500\n",
            "Total training time: 49.97 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 9346.01, NNZs: 278, Bias: -24.130276, T: 16193196, Avg. loss: 151.750030\n",
            "Total training time: 50.08 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 9346.13, NNZs: 284, Bias: -24.164721, T: 16227144, Avg. loss: 149.543023\n",
            "Total training time: 50.19 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 9346.27, NNZs: 275, Bias: -24.191774, T: 16261092, Avg. loss: 151.188823\n",
            "Total training time: 50.31 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 9346.40, NNZs: 275, Bias: -24.231637, T: 16295040, Avg. loss: 146.365950\n",
            "Total training time: 50.42 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 9346.53, NNZs: 286, Bias: -24.269485, T: 16328988, Avg. loss: 148.170886\n",
            "Total training time: 50.53 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 9346.66, NNZs: 280, Bias: -24.290005, T: 16362936, Avg. loss: 151.480078\n",
            "Total training time: 50.65 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 9346.79, NNZs: 273, Bias: -24.327098, T: 16396884, Avg. loss: 149.591598\n",
            "Total training time: 50.76 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 9346.92, NNZs: 276, Bias: -24.356028, T: 16430832, Avg. loss: 149.198202\n",
            "Total training time: 50.88 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 9347.05, NNZs: 282, Bias: -24.386740, T: 16464780, Avg. loss: 149.320777\n",
            "Total training time: 50.99 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 9347.18, NNZs: 279, Bias: -24.422134, T: 16498728, Avg. loss: 147.231200\n",
            "Total training time: 51.10 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 9347.30, NNZs: 274, Bias: -24.448404, T: 16532676, Avg. loss: 147.984526\n",
            "Total training time: 51.21 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 9347.43, NNZs: 277, Bias: -24.469467, T: 16566624, Avg. loss: 146.411595\n",
            "Total training time: 51.32 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 9347.56, NNZs: 277, Bias: -24.495687, T: 16600572, Avg. loss: 150.777608\n",
            "Total training time: 51.43 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 9347.68, NNZs: 275, Bias: -24.521784, T: 16634520, Avg. loss: 147.468950\n",
            "Total training time: 51.53 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 9347.81, NNZs: 284, Bias: -24.552837, T: 16668468, Avg. loss: 149.950628\n",
            "Total training time: 51.65 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 9347.92, NNZs: 278, Bias: -24.587986, T: 16702416, Avg. loss: 149.726014\n",
            "Total training time: 51.76 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 9348.04, NNZs: 280, Bias: -24.625620, T: 16736364, Avg. loss: 149.118391\n",
            "Total training time: 51.87 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 9348.17, NNZs: 285, Bias: -24.656679, T: 16770312, Avg. loss: 145.527522\n",
            "Total training time: 51.98 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 9348.30, NNZs: 275, Bias: -24.684904, T: 16804260, Avg. loss: 143.403149\n",
            "Total training time: 52.09 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 9348.41, NNZs: 292, Bias: -24.711001, T: 16838208, Avg. loss: 146.188439\n",
            "Total training time: 52.20 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 9348.52, NNZs: 285, Bias: -24.739602, T: 16872156, Avg. loss: 151.167576\n",
            "Total training time: 52.31 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 9348.64, NNZs: 282, Bias: -24.774472, T: 16906104, Avg. loss: 149.082550\n",
            "Total training time: 52.42 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 9348.75, NNZs: 285, Bias: -24.796145, T: 16940052, Avg. loss: 146.665718\n",
            "Total training time: 52.53 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 9348.87, NNZs: 282, Bias: -24.823417, T: 16974000, Avg. loss: 145.411834\n",
            "Total training time: 52.64 seconds.\n",
            "-- Epoch 1\n",
            "Norm: 22698.47, NNZs: 360, Bias: -124.425731, T: 33948, Avg. loss: 580740.431536\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13837.38, NNZs: 365, Bias: -152.622207, T: 67896, Avg. loss: 118850.264388\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 10133.77, NNZs: 363, Bias: -169.663374, T: 101844, Avg. loss: 68332.954849\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 8561.95, NNZs: 363, Bias: -181.155829, T: 135792, Avg. loss: 45082.207018\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 7935.82, NNZs: 360, Bias: -189.309376, T: 169740, Avg. loss: 33964.361822\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 7684.11, NNZs: 359, Bias: -196.801576, T: 203688, Avg. loss: 26897.585001\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 7642.81, NNZs: 355, Bias: -201.999582, T: 237636, Avg. loss: 21465.718661\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 7696.99, NNZs: 352, Bias: -205.585921, T: 271584, Avg. loss: 18581.927496\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 7779.14, NNZs: 356, Bias: -208.789345, T: 305532, Avg. loss: 16102.955910\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 7904.75, NNZs: 349, Bias: -212.274776, T: 339480, Avg. loss: 14202.005071\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 8017.08, NNZs: 350, Bias: -214.712751, T: 373428, Avg. loss: 12659.484626\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 8129.97, NNZs: 356, Bias: -216.751607, T: 407376, Avg. loss: 11530.751753\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 8229.40, NNZs: 348, Bias: -219.202990, T: 441324, Avg. loss: 10470.925082\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 8332.69, NNZs: 350, Bias: -221.189148, T: 475272, Avg. loss: 9306.106839\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 8424.54, NNZs: 344, Bias: -223.048586, T: 509220, Avg. loss: 8459.372442\n",
            "Total training time: 1.53 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 8509.02, NNZs: 347, Bias: -224.612366, T: 543168, Avg. loss: 8058.916486\n",
            "Total training time: 1.64 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 8591.46, NNZs: 340, Bias: -226.093208, T: 577116, Avg. loss: 7520.515382\n",
            "Total training time: 1.74 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 8672.08, NNZs: 328, Bias: -227.415268, T: 611064, Avg. loss: 7032.675331\n",
            "Total training time: 1.84 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 8746.07, NNZs: 334, Bias: -228.715370, T: 645012, Avg. loss: 6639.074249\n",
            "Total training time: 1.95 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 8811.11, NNZs: 336, Bias: -229.894081, T: 678960, Avg. loss: 6195.864649\n",
            "Total training time: 2.05 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 8871.89, NNZs: 325, Bias: -230.747792, T: 712908, Avg. loss: 5905.493057\n",
            "Total training time: 2.16 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 8932.92, NNZs: 336, Bias: -231.947884, T: 746856, Avg. loss: 5638.077442\n",
            "Total training time: 2.26 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 8989.00, NNZs: 329, Bias: -233.167048, T: 780804, Avg. loss: 5277.648122\n",
            "Total training time: 2.37 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 9043.27, NNZs: 334, Bias: -234.305483, T: 814752, Avg. loss: 4931.613227\n",
            "Total training time: 2.47 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 9092.61, NNZs: 327, Bias: -235.284287, T: 848700, Avg. loss: 4715.923627\n",
            "Total training time: 2.58 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 9141.03, NNZs: 331, Bias: -236.180955, T: 882648, Avg. loss: 4502.898240\n",
            "Total training time: 2.68 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 9188.51, NNZs: 334, Bias: -236.876487, T: 916596, Avg. loss: 4240.748958\n",
            "Total training time: 2.79 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 9229.79, NNZs: 329, Bias: -237.559083, T: 950544, Avg. loss: 4070.323265\n",
            "Total training time: 2.89 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 9271.36, NNZs: 320, Bias: -238.268259, T: 984492, Avg. loss: 3925.758476\n",
            "Total training time: 2.99 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 9313.89, NNZs: 317, Bias: -238.983615, T: 1018440, Avg. loss: 3676.739823\n",
            "Total training time: 3.10 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 9353.42, NNZs: 322, Bias: -239.492237, T: 1052388, Avg. loss: 3572.860344\n",
            "Total training time: 3.20 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 9388.99, NNZs: 325, Bias: -240.209806, T: 1086336, Avg. loss: 3456.280938\n",
            "Total training time: 3.30 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 9424.65, NNZs: 324, Bias: -240.711104, T: 1120284, Avg. loss: 3260.782322\n",
            "Total training time: 3.41 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 9457.35, NNZs: 322, Bias: -241.247571, T: 1154232, Avg. loss: 3263.393772\n",
            "Total training time: 3.51 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 9487.07, NNZs: 323, Bias: -241.822825, T: 1188180, Avg. loss: 3175.684574\n",
            "Total training time: 3.61 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 9519.20, NNZs: 315, Bias: -242.304738, T: 1222128, Avg. loss: 2990.334508\n",
            "Total training time: 3.72 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 9550.10, NNZs: 313, Bias: -242.853170, T: 1256076, Avg. loss: 2853.334822\n",
            "Total training time: 3.82 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 9581.30, NNZs: 314, Bias: -243.212061, T: 1290024, Avg. loss: 2670.530626\n",
            "Total training time: 3.93 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 9611.67, NNZs: 308, Bias: -243.787047, T: 1323972, Avg. loss: 2620.684926\n",
            "Total training time: 4.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 9638.67, NNZs: 302, Bias: -244.156956, T: 1357920, Avg. loss: 2523.765227\n",
            "Total training time: 4.14 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 9665.26, NNZs: 307, Bias: -244.693062, T: 1391868, Avg. loss: 2494.060332\n",
            "Total training time: 4.25 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 9690.69, NNZs: 310, Bias: -245.139746, T: 1425816, Avg. loss: 2404.371560\n",
            "Total training time: 4.35 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 9716.68, NNZs: 308, Bias: -245.461606, T: 1459764, Avg. loss: 2314.839086\n",
            "Total training time: 4.46 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 9739.24, NNZs: 306, Bias: -245.908433, T: 1493712, Avg. loss: 2285.753038\n",
            "Total training time: 4.56 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 9762.32, NNZs: 316, Bias: -246.258750, T: 1527660, Avg. loss: 2233.849813\n",
            "Total training time: 4.66 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 9785.57, NNZs: 303, Bias: -246.723148, T: 1561608, Avg. loss: 2105.860604\n",
            "Total training time: 4.77 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 9807.61, NNZs: 306, Bias: -247.291654, T: 1595556, Avg. loss: 2057.800724\n",
            "Total training time: 4.87 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 9827.84, NNZs: 313, Bias: -247.705553, T: 1629504, Avg. loss: 2004.695013\n",
            "Total training time: 4.98 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 9848.01, NNZs: 313, Bias: -248.133150, T: 1663452, Avg. loss: 1972.245217\n",
            "Total training time: 5.08 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 9867.45, NNZs: 313, Bias: -248.495445, T: 1697400, Avg. loss: 1957.829993\n",
            "Total training time: 5.18 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 9886.23, NNZs: 318, Bias: -248.881696, T: 1731348, Avg. loss: 1891.314074\n",
            "Total training time: 5.29 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 9903.29, NNZs: 307, Bias: -249.031130, T: 1765296, Avg. loss: 1886.582301\n",
            "Total training time: 5.39 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 9921.56, NNZs: 307, Bias: -249.428127, T: 1799244, Avg. loss: 1778.654710\n",
            "Total training time: 5.49 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 9938.75, NNZs: 304, Bias: -249.731929, T: 1833192, Avg. loss: 1765.642246\n",
            "Total training time: 5.59 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 9955.76, NNZs: 305, Bias: -250.062971, T: 1867140, Avg. loss: 1688.476757\n",
            "Total training time: 5.69 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 9971.75, NNZs: 308, Bias: -250.364617, T: 1901088, Avg. loss: 1711.591105\n",
            "Total training time: 5.79 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 9987.04, NNZs: 308, Bias: -250.764246, T: 1935036, Avg. loss: 1639.593108\n",
            "Total training time: 5.89 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 10001.88, NNZs: 305, Bias: -251.112939, T: 1968984, Avg. loss: 1587.764010\n",
            "Total training time: 5.99 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 10016.79, NNZs: 303, Bias: -251.420896, T: 2002932, Avg. loss: 1575.265575\n",
            "Total training time: 6.09 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 10030.88, NNZs: 297, Bias: -251.767268, T: 2036880, Avg. loss: 1542.233797\n",
            "Total training time: 6.20 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 10044.98, NNZs: 297, Bias: -251.962235, T: 2070828, Avg. loss: 1501.998900\n",
            "Total training time: 6.30 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 10058.43, NNZs: 296, Bias: -252.278093, T: 2104776, Avg. loss: 1495.177476\n",
            "Total training time: 6.40 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 10071.86, NNZs: 301, Bias: -252.542927, T: 2138724, Avg. loss: 1461.922066\n",
            "Total training time: 6.50 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 10084.15, NNZs: 297, Bias: -252.901634, T: 2172672, Avg. loss: 1429.371185\n",
            "Total training time: 6.60 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 10097.57, NNZs: 307, Bias: -253.151299, T: 2206620, Avg. loss: 1327.953071\n",
            "Total training time: 6.69 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 10109.35, NNZs: 310, Bias: -253.474920, T: 2240568, Avg. loss: 1376.654920\n",
            "Total training time: 6.79 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 10120.27, NNZs: 304, Bias: -253.744132, T: 2274516, Avg. loss: 1419.129617\n",
            "Total training time: 6.89 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 10131.50, NNZs: 305, Bias: -254.063537, T: 2308464, Avg. loss: 1349.710906\n",
            "Total training time: 6.99 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 10142.26, NNZs: 302, Bias: -254.341673, T: 2342412, Avg. loss: 1345.165458\n",
            "Total training time: 7.10 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 10152.87, NNZs: 296, Bias: -254.663757, T: 2376360, Avg. loss: 1249.557229\n",
            "Total training time: 7.20 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 10163.49, NNZs: 308, Bias: -254.878653, T: 2410308, Avg. loss: 1263.213732\n",
            "Total training time: 7.30 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 10173.43, NNZs: 311, Bias: -255.142387, T: 2444256, Avg. loss: 1268.294432\n",
            "Total training time: 7.40 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 10182.48, NNZs: 306, Bias: -255.410243, T: 2478204, Avg. loss: 1261.690236\n",
            "Total training time: 7.50 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 10191.93, NNZs: 299, Bias: -255.589974, T: 2512152, Avg. loss: 1233.351368\n",
            "Total training time: 7.60 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 10201.17, NNZs: 298, Bias: -255.828611, T: 2546100, Avg. loss: 1219.634528\n",
            "Total training time: 7.73 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 10210.49, NNZs: 291, Bias: -256.039555, T: 2580048, Avg. loss: 1162.777963\n",
            "Total training time: 7.83 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 10219.36, NNZs: 291, Bias: -256.317850, T: 2613996, Avg. loss: 1145.772306\n",
            "Total training time: 7.93 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 10228.13, NNZs: 304, Bias: -256.480940, T: 2647944, Avg. loss: 1116.565884\n",
            "Total training time: 8.03 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 10237.02, NNZs: 294, Bias: -256.710446, T: 2681892, Avg. loss: 1102.427554\n",
            "Total training time: 8.13 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 10245.06, NNZs: 292, Bias: -256.886491, T: 2715840, Avg. loss: 1113.111571\n",
            "Total training time: 8.24 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 10253.12, NNZs: 286, Bias: -256.980736, T: 2749788, Avg. loss: 1095.719991\n",
            "Total training time: 8.34 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 10262.00, NNZs: 293, Bias: -257.197570, T: 2783736, Avg. loss: 1000.498806\n",
            "Total training time: 8.44 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 10270.10, NNZs: 296, Bias: -257.346891, T: 2817684, Avg. loss: 1033.189129\n",
            "Total training time: 8.55 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 10277.73, NNZs: 298, Bias: -257.503414, T: 2851632, Avg. loss: 1033.327770\n",
            "Total training time: 8.65 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 10285.34, NNZs: 282, Bias: -257.697590, T: 2885580, Avg. loss: 976.876982\n",
            "Total training time: 8.75 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 10292.97, NNZs: 287, Bias: -257.892041, T: 2919528, Avg. loss: 986.930459\n",
            "Total training time: 8.85 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 10300.40, NNZs: 291, Bias: -258.104182, T: 2953476, Avg. loss: 990.756934\n",
            "Total training time: 8.96 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 10307.26, NNZs: 293, Bias: -258.290277, T: 2987424, Avg. loss: 970.663717\n",
            "Total training time: 9.06 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 10314.21, NNZs: 284, Bias: -258.464902, T: 3021372, Avg. loss: 963.812746\n",
            "Total training time: 9.16 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 10320.95, NNZs: 284, Bias: -258.644054, T: 3055320, Avg. loss: 958.616932\n",
            "Total training time: 9.27 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 10327.85, NNZs: 287, Bias: -258.780374, T: 3089268, Avg. loss: 903.389897\n",
            "Total training time: 9.37 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 10334.66, NNZs: 296, Bias: -258.804646, T: 3123216, Avg. loss: 899.274634\n",
            "Total training time: 9.47 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 10340.91, NNZs: 292, Bias: -258.974515, T: 3157164, Avg. loss: 904.980884\n",
            "Total training time: 9.58 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 10347.35, NNZs: 297, Bias: -259.129729, T: 3191112, Avg. loss: 906.298971\n",
            "Total training time: 9.68 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 10353.04, NNZs: 293, Bias: -259.357073, T: 3225060, Avg. loss: 899.171152\n",
            "Total training time: 9.78 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 10358.70, NNZs: 300, Bias: -259.576746, T: 3259008, Avg. loss: 909.170968\n",
            "Total training time: 9.89 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 10364.33, NNZs: 292, Bias: -259.742302, T: 3292956, Avg. loss: 887.762902\n",
            "Total training time: 9.99 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 10369.88, NNZs: 290, Bias: -259.907533, T: 3326904, Avg. loss: 886.947436\n",
            "Total training time: 10.10 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 10375.37, NNZs: 297, Bias: -260.096940, T: 3360852, Avg. loss: 871.682675\n",
            "Total training time: 10.20 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 10380.81, NNZs: 284, Bias: -260.245713, T: 3394800, Avg. loss: 869.382591\n",
            "Total training time: 10.30 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 10386.31, NNZs: 279, Bias: -260.395430, T: 3428748, Avg. loss: 823.185222\n",
            "Total training time: 10.41 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 10391.70, NNZs: 283, Bias: -260.524060, T: 3462696, Avg. loss: 812.277012\n",
            "Total training time: 10.51 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 10396.92, NNZs: 295, Bias: -260.638830, T: 3496644, Avg. loss: 821.504042\n",
            "Total training time: 10.61 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 10402.03, NNZs: 288, Bias: -260.763981, T: 3530592, Avg. loss: 781.870284\n",
            "Total training time: 10.72 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 10407.09, NNZs: 290, Bias: -260.944136, T: 3564540, Avg. loss: 802.973841\n",
            "Total training time: 10.82 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 10411.71, NNZs: 294, Bias: -261.104466, T: 3598488, Avg. loss: 809.093358\n",
            "Total training time: 10.92 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 10416.43, NNZs: 286, Bias: -261.217029, T: 3632436, Avg. loss: 811.686202\n",
            "Total training time: 11.03 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 10421.06, NNZs: 294, Bias: -261.410518, T: 3666384, Avg. loss: 785.229332\n",
            "Total training time: 11.14 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 10425.29, NNZs: 286, Bias: -261.575611, T: 3700332, Avg. loss: 814.138547\n",
            "Total training time: 11.25 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 10429.73, NNZs: 294, Bias: -261.694251, T: 3734280, Avg. loss: 769.213981\n",
            "Total training time: 11.36 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 10434.03, NNZs: 288, Bias: -261.821905, T: 3768228, Avg. loss: 774.755453\n",
            "Total training time: 11.46 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 10438.38, NNZs: 299, Bias: -261.920498, T: 3802176, Avg. loss: 749.840831\n",
            "Total training time: 11.56 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 10442.50, NNZs: 290, Bias: -262.023729, T: 3836124, Avg. loss: 750.271192\n",
            "Total training time: 11.67 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 10446.92, NNZs: 293, Bias: -262.170693, T: 3870072, Avg. loss: 709.801714\n",
            "Total training time: 11.77 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 10450.95, NNZs: 287, Bias: -262.311814, T: 3904020, Avg. loss: 744.769802\n",
            "Total training time: 11.87 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 10455.04, NNZs: 294, Bias: -262.395607, T: 3937968, Avg. loss: 716.499910\n",
            "Total training time: 11.97 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 10458.84, NNZs: 288, Bias: -262.510271, T: 3971916, Avg. loss: 756.072204\n",
            "Total training time: 12.08 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 10462.77, NNZs: 289, Bias: -262.653049, T: 4005864, Avg. loss: 691.491636\n",
            "Total training time: 12.18 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 10466.57, NNZs: 295, Bias: -262.778401, T: 4039812, Avg. loss: 714.829902\n",
            "Total training time: 12.28 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 10470.10, NNZs: 299, Bias: -262.959397, T: 4073760, Avg. loss: 725.184582\n",
            "Total training time: 12.38 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 10473.73, NNZs: 282, Bias: -263.063900, T: 4107708, Avg. loss: 690.247734\n",
            "Total training time: 12.49 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 10477.34, NNZs: 293, Bias: -263.150936, T: 4141656, Avg. loss: 703.134628\n",
            "Total training time: 12.59 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 10480.67, NNZs: 298, Bias: -263.233764, T: 4175604, Avg. loss: 715.439287\n",
            "Total training time: 12.69 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 10483.96, NNZs: 290, Bias: -263.316087, T: 4209552, Avg. loss: 710.766751\n",
            "Total training time: 12.79 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 10487.15, NNZs: 294, Bias: -263.383972, T: 4243500, Avg. loss: 691.938585\n",
            "Total training time: 12.90 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 10490.43, NNZs: 291, Bias: -263.476383, T: 4277448, Avg. loss: 692.197471\n",
            "Total training time: 13.00 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 10493.66, NNZs: 288, Bias: -263.580984, T: 4311396, Avg. loss: 660.029503\n",
            "Total training time: 13.10 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 10497.08, NNZs: 288, Bias: -263.663297, T: 4345344, Avg. loss: 658.696043\n",
            "Total training time: 13.20 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 10500.24, NNZs: 293, Bias: -263.739377, T: 4379292, Avg. loss: 655.050069\n",
            "Total training time: 13.31 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 10503.53, NNZs: 295, Bias: -263.808707, T: 4413240, Avg. loss: 652.490835\n",
            "Total training time: 13.41 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 10506.55, NNZs: 297, Bias: -263.877637, T: 4447188, Avg. loss: 668.707216\n",
            "Total training time: 13.52 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 10509.57, NNZs: 293, Bias: -263.942460, T: 4481136, Avg. loss: 659.442877\n",
            "Total training time: 13.62 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 10512.47, NNZs: 290, Bias: -264.022172, T: 4515084, Avg. loss: 647.816898\n",
            "Total training time: 13.72 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 10515.40, NNZs: 293, Bias: -264.138792, T: 4549032, Avg. loss: 614.828488\n",
            "Total training time: 13.82 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 10518.20, NNZs: 288, Bias: -264.238919, T: 4582980, Avg. loss: 635.459308\n",
            "Total training time: 13.92 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 10521.04, NNZs: 296, Bias: -264.322329, T: 4616928, Avg. loss: 638.329197\n",
            "Total training time: 14.03 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 10523.76, NNZs: 303, Bias: -264.387803, T: 4650876, Avg. loss: 633.932433\n",
            "Total training time: 14.13 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 10526.36, NNZs: 299, Bias: -264.532134, T: 4684824, Avg. loss: 649.339069\n",
            "Total training time: 14.23 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 10528.92, NNZs: 298, Bias: -264.591854, T: 4718772, Avg. loss: 632.282861\n",
            "Total training time: 14.33 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 10531.49, NNZs: 296, Bias: -264.669752, T: 4752720, Avg. loss: 628.060335\n",
            "Total training time: 14.43 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 10534.04, NNZs: 295, Bias: -264.759859, T: 4786668, Avg. loss: 600.228649\n",
            "Total training time: 14.54 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 10536.63, NNZs: 291, Bias: -264.820298, T: 4820616, Avg. loss: 607.349239\n",
            "Total training time: 14.64 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 10539.06, NNZs: 284, Bias: -264.922568, T: 4854564, Avg. loss: 602.759990\n",
            "Total training time: 14.75 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 10541.60, NNZs: 284, Bias: -265.004083, T: 4888512, Avg. loss: 587.454189\n",
            "Total training time: 14.85 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 10544.11, NNZs: 289, Bias: -265.096870, T: 4922460, Avg. loss: 592.238064\n",
            "Total training time: 14.95 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 10546.63, NNZs: 295, Bias: -265.138882, T: 4956408, Avg. loss: 588.696938\n",
            "Total training time: 15.06 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 10549.06, NNZs: 291, Bias: -265.200307, T: 4990356, Avg. loss: 583.896648\n",
            "Total training time: 15.16 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 10551.26, NNZs: 300, Bias: -265.252341, T: 5024304, Avg. loss: 587.784831\n",
            "Total training time: 15.27 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 10553.55, NNZs: 293, Bias: -265.329495, T: 5058252, Avg. loss: 586.311929\n",
            "Total training time: 15.37 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 10555.85, NNZs: 299, Bias: -265.365684, T: 5092200, Avg. loss: 568.460896\n",
            "Total training time: 15.47 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 10558.16, NNZs: 285, Bias: -265.402553, T: 5126148, Avg. loss: 562.035002\n",
            "Total training time: 15.57 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 10560.49, NNZs: 284, Bias: -265.450404, T: 5160096, Avg. loss: 541.844989\n",
            "Total training time: 15.68 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 10562.95, NNZs: 288, Bias: -265.481361, T: 5194044, Avg. loss: 514.274726\n",
            "Total training time: 15.78 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 10565.21, NNZs: 288, Bias: -265.543253, T: 5227992, Avg. loss: 548.733754\n",
            "Total training time: 15.89 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 10567.46, NNZs: 279, Bias: -265.614242, T: 5261940, Avg. loss: 527.047278\n",
            "Total training time: 15.99 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 10569.56, NNZs: 280, Bias: -265.626065, T: 5295888, Avg. loss: 537.585332\n",
            "Total training time: 16.10 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 10571.79, NNZs: 286, Bias: -265.704662, T: 5329836, Avg. loss: 500.415535\n",
            "Total training time: 16.20 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 10573.88, NNZs: 283, Bias: -265.753339, T: 5363784, Avg. loss: 545.719204\n",
            "Total training time: 16.31 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 10575.99, NNZs: 284, Bias: -265.822480, T: 5397732, Avg. loss: 513.685301\n",
            "Total training time: 16.42 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 10578.13, NNZs: 283, Bias: -265.864109, T: 5431680, Avg. loss: 514.241625\n",
            "Total training time: 16.53 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 10580.14, NNZs: 286, Bias: -265.919622, T: 5465628, Avg. loss: 529.390294\n",
            "Total training time: 16.64 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 10582.08, NNZs: 285, Bias: -265.969947, T: 5499576, Avg. loss: 519.505681\n",
            "Total training time: 16.75 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 10584.07, NNZs: 284, Bias: -266.015884, T: 5533524, Avg. loss: 515.582847\n",
            "Total training time: 16.85 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 10586.04, NNZs: 289, Bias: -266.044275, T: 5567472, Avg. loss: 511.965532\n",
            "Total training time: 16.96 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 10587.93, NNZs: 289, Bias: -266.086849, T: 5601420, Avg. loss: 505.323311\n",
            "Total training time: 17.07 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 10589.83, NNZs: 290, Bias: -266.145587, T: 5635368, Avg. loss: 497.717342\n",
            "Total training time: 17.17 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 10591.67, NNZs: 285, Bias: -266.211930, T: 5669316, Avg. loss: 503.117516\n",
            "Total training time: 17.28 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 10593.46, NNZs: 286, Bias: -266.220259, T: 5703264, Avg. loss: 496.390202\n",
            "Total training time: 17.38 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 10595.30, NNZs: 286, Bias: -266.248496, T: 5737212, Avg. loss: 488.930563\n",
            "Total training time: 17.49 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 10597.14, NNZs: 288, Bias: -266.289517, T: 5771160, Avg. loss: 490.478152\n",
            "Total training time: 17.59 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 10598.78, NNZs: 294, Bias: -266.315710, T: 5805108, Avg. loss: 512.499227\n",
            "Total training time: 17.70 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 10600.50, NNZs: 290, Bias: -266.329056, T: 5839056, Avg. loss: 492.506857\n",
            "Total training time: 17.80 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 10602.31, NNZs: 285, Bias: -266.358755, T: 5873004, Avg. loss: 480.441523\n",
            "Total training time: 17.91 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 10604.06, NNZs: 297, Bias: -266.386473, T: 5906952, Avg. loss: 470.463019\n",
            "Total training time: 18.02 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 10605.72, NNZs: 288, Bias: -266.416656, T: 5940900, Avg. loss: 475.162047\n",
            "Total training time: 18.12 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 10607.40, NNZs: 285, Bias: -266.472846, T: 5974848, Avg. loss: 471.198114\n",
            "Total training time: 18.23 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 10609.00, NNZs: 281, Bias: -266.487163, T: 6008796, Avg. loss: 470.821386\n",
            "Total training time: 18.33 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 10610.61, NNZs: 276, Bias: -266.524196, T: 6042744, Avg. loss: 471.827703\n",
            "Total training time: 18.44 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 10612.33, NNZs: 285, Bias: -266.558404, T: 6076692, Avg. loss: 458.788658\n",
            "Total training time: 18.54 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 10613.93, NNZs: 287, Bias: -266.582580, T: 6110640, Avg. loss: 459.532690\n",
            "Total training time: 18.65 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 10615.53, NNZs: 288, Bias: -266.641697, T: 6144588, Avg. loss: 454.648990\n",
            "Total training time: 18.75 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 10617.04, NNZs: 287, Bias: -266.674494, T: 6178536, Avg. loss: 469.659223\n",
            "Total training time: 18.85 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 10618.67, NNZs: 293, Bias: -266.707650, T: 6212484, Avg. loss: 454.424243\n",
            "Total training time: 18.96 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 10620.13, NNZs: 300, Bias: -266.712989, T: 6246432, Avg. loss: 452.524995\n",
            "Total training time: 19.06 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 10621.60, NNZs: 291, Bias: -266.735319, T: 6280380, Avg. loss: 462.451552\n",
            "Total training time: 19.16 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 10623.13, NNZs: 286, Bias: -266.741376, T: 6314328, Avg. loss: 460.184804\n",
            "Total training time: 19.27 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 10624.59, NNZs: 284, Bias: -266.787670, T: 6348276, Avg. loss: 444.433879\n",
            "Total training time: 19.37 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 10626.08, NNZs: 283, Bias: -266.796608, T: 6382224, Avg. loss: 435.563641\n",
            "Total training time: 19.48 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 10627.48, NNZs: 283, Bias: -266.823080, T: 6416172, Avg. loss: 440.126324\n",
            "Total training time: 19.58 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 10628.83, NNZs: 293, Bias: -266.853210, T: 6450120, Avg. loss: 445.668791\n",
            "Total training time: 19.68 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 10630.16, NNZs: 285, Bias: -266.895515, T: 6484068, Avg. loss: 447.099151\n",
            "Total training time: 19.79 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 10631.53, NNZs: 298, Bias: -266.928332, T: 6518016, Avg. loss: 443.529211\n",
            "Total training time: 19.90 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 10632.87, NNZs: 284, Bias: -266.965628, T: 6551964, Avg. loss: 421.385001\n",
            "Total training time: 20.01 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 10634.24, NNZs: 289, Bias: -266.977884, T: 6585912, Avg. loss: 441.015242\n",
            "Total training time: 20.12 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 10635.58, NNZs: 295, Bias: -267.011753, T: 6619860, Avg. loss: 426.047897\n",
            "Total training time: 20.22 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 10636.96, NNZs: 283, Bias: -267.033487, T: 6653808, Avg. loss: 428.924009\n",
            "Total training time: 20.33 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 10638.35, NNZs: 287, Bias: -267.051497, T: 6687756, Avg. loss: 418.526449\n",
            "Total training time: 20.44 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 10639.59, NNZs: 290, Bias: -267.088972, T: 6721704, Avg. loss: 425.039573\n",
            "Total training time: 20.54 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 10640.81, NNZs: 294, Bias: -267.102839, T: 6755652, Avg. loss: 421.587137\n",
            "Total training time: 20.64 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 10642.09, NNZs: 295, Bias: -267.136380, T: 6789600, Avg. loss: 417.721486\n",
            "Total training time: 20.75 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 10643.27, NNZs: 284, Bias: -267.173895, T: 6823548, Avg. loss: 426.704284\n",
            "Total training time: 20.86 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 10644.48, NNZs: 281, Bias: -267.180868, T: 6857496, Avg. loss: 412.711735\n",
            "Total training time: 20.96 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 10645.73, NNZs: 285, Bias: -267.197950, T: 6891444, Avg. loss: 411.779724\n",
            "Total training time: 21.07 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 10647.02, NNZs: 268, Bias: -267.193533, T: 6925392, Avg. loss: 394.696290\n",
            "Total training time: 21.17 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 10648.32, NNZs: 285, Bias: -267.179872, T: 6959340, Avg. loss: 389.663226\n",
            "Total training time: 21.27 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 10649.56, NNZs: 290, Bias: -267.186396, T: 6993288, Avg. loss: 399.707612\n",
            "Total training time: 21.37 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 10650.68, NNZs: 280, Bias: -267.212214, T: 7027236, Avg. loss: 417.815670\n",
            "Total training time: 21.48 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 10651.89, NNZs: 286, Bias: -267.225492, T: 7061184, Avg. loss: 405.064937\n",
            "Total training time: 21.58 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 10653.10, NNZs: 288, Bias: -267.229283, T: 7095132, Avg. loss: 386.726278\n",
            "Total training time: 21.69 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 10654.25, NNZs: 286, Bias: -267.238933, T: 7129080, Avg. loss: 406.846548\n",
            "Total training time: 21.79 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 10655.43, NNZs: 286, Bias: -267.254078, T: 7163028, Avg. loss: 392.232594\n",
            "Total training time: 21.90 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 10656.54, NNZs: 284, Bias: -267.255706, T: 7196976, Avg. loss: 402.411204\n",
            "Total training time: 22.00 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 10657.63, NNZs: 289, Bias: -267.295488, T: 7230924, Avg. loss: 386.802537\n",
            "Total training time: 22.10 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 10658.75, NNZs: 285, Bias: -267.304995, T: 7264872, Avg. loss: 395.154101\n",
            "Total training time: 22.21 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 10659.87, NNZs: 285, Bias: -267.324784, T: 7298820, Avg. loss: 389.285487\n",
            "Total training time: 22.32 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 10660.90, NNZs: 290, Bias: -267.330619, T: 7332768, Avg. loss: 400.747308\n",
            "Total training time: 22.43 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 10661.94, NNZs: 287, Bias: -267.344948, T: 7366716, Avg. loss: 398.557652\n",
            "Total training time: 22.54 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 10663.01, NNZs: 286, Bias: -267.360010, T: 7400664, Avg. loss: 376.628341\n",
            "Total training time: 22.66 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 10664.07, NNZs: 273, Bias: -267.393017, T: 7434612, Avg. loss: 369.701101\n",
            "Total training time: 22.77 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 10665.13, NNZs: 281, Bias: -267.403886, T: 7468560, Avg. loss: 370.669395\n",
            "Total training time: 22.88 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 10666.19, NNZs: 280, Bias: -267.408523, T: 7502508, Avg. loss: 356.591686\n",
            "Total training time: 22.99 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 10667.25, NNZs: 284, Bias: -267.392624, T: 7536456, Avg. loss: 357.404539\n",
            "Total training time: 23.10 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 10668.24, NNZs: 279, Bias: -267.409976, T: 7570404, Avg. loss: 374.351894\n",
            "Total training time: 23.21 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 10669.27, NNZs: 280, Bias: -267.399354, T: 7604352, Avg. loss: 368.914385\n",
            "Total training time: 23.32 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 10670.27, NNZs: 277, Bias: -267.412747, T: 7638300, Avg. loss: 374.631956\n",
            "Total training time: 23.44 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 10671.28, NNZs: 289, Bias: -267.409863, T: 7672248, Avg. loss: 366.923761\n",
            "Total training time: 23.55 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 10672.22, NNZs: 290, Bias: -267.412781, T: 7706196, Avg. loss: 365.508011\n",
            "Total training time: 23.67 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 10673.17, NNZs: 294, Bias: -267.425179, T: 7740144, Avg. loss: 372.997821\n",
            "Total training time: 23.78 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 10674.14, NNZs: 286, Bias: -267.439109, T: 7774092, Avg. loss: 353.434866\n",
            "Total training time: 23.89 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 10675.09, NNZs: 282, Bias: -267.448730, T: 7808040, Avg. loss: 357.313766\n",
            "Total training time: 24.00 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 10676.04, NNZs: 285, Bias: -267.430246, T: 7841988, Avg. loss: 362.545693\n",
            "Total training time: 24.12 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 10676.98, NNZs: 297, Bias: -267.439038, T: 7875936, Avg. loss: 363.719575\n",
            "Total training time: 24.23 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 10677.87, NNZs: 288, Bias: -267.454090, T: 7909884, Avg. loss: 360.754124\n",
            "Total training time: 24.34 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 10678.76, NNZs: 274, Bias: -267.452886, T: 7943832, Avg. loss: 362.596169\n",
            "Total training time: 24.46 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 10679.69, NNZs: 285, Bias: -267.457640, T: 7977780, Avg. loss: 337.839398\n",
            "Total training time: 24.57 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 10680.54, NNZs: 300, Bias: -267.452727, T: 8011728, Avg. loss: 363.682217\n",
            "Total training time: 24.68 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 10681.38, NNZs: 297, Bias: -267.466452, T: 8045676, Avg. loss: 362.619667\n",
            "Total training time: 24.79 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 10682.20, NNZs: 286, Bias: -267.489308, T: 8079624, Avg. loss: 366.395996\n",
            "Total training time: 24.90 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 10683.07, NNZs: 289, Bias: -267.487094, T: 8113572, Avg. loss: 353.117443\n",
            "Total training time: 25.01 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 10683.91, NNZs: 285, Bias: -267.482292, T: 8147520, Avg. loss: 341.136016\n",
            "Total training time: 25.12 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 10684.78, NNZs: 281, Bias: -267.499655, T: 8181468, Avg. loss: 339.430848\n",
            "Total training time: 25.24 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 10685.63, NNZs: 281, Bias: -267.482884, T: 8215416, Avg. loss: 337.655665\n",
            "Total training time: 25.35 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 10686.50, NNZs: 290, Bias: -267.470565, T: 8249364, Avg. loss: 338.657967\n",
            "Total training time: 25.46 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 10687.34, NNZs: 286, Bias: -267.450610, T: 8283312, Avg. loss: 341.895496\n",
            "Total training time: 25.58 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 10688.14, NNZs: 287, Bias: -267.457102, T: 8317260, Avg. loss: 345.776991\n",
            "Total training time: 25.69 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 10688.96, NNZs: 278, Bias: -267.448337, T: 8351208, Avg. loss: 341.431197\n",
            "Total training time: 25.81 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 10689.75, NNZs: 285, Bias: -267.439253, T: 8385156, Avg. loss: 343.271094\n",
            "Total training time: 25.92 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 10690.57, NNZs: 286, Bias: -267.427009, T: 8419104, Avg. loss: 326.197719\n",
            "Total training time: 26.03 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 10691.33, NNZs: 285, Bias: -267.409448, T: 8453052, Avg. loss: 341.382680\n",
            "Total training time: 26.15 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 10692.13, NNZs: 274, Bias: -267.414492, T: 8487000, Avg. loss: 324.328781\n",
            "Total training time: 26.27 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 10692.97, NNZs: 288, Bias: -267.402726, T: 8520948, Avg. loss: 321.662992\n",
            "Total training time: 26.38 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 10693.71, NNZs: 293, Bias: -267.415859, T: 8554896, Avg. loss: 333.180509\n",
            "Total training time: 26.49 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 10694.50, NNZs: 284, Bias: -267.427080, T: 8588844, Avg. loss: 324.215399\n",
            "Total training time: 26.61 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 10695.29, NNZs: 276, Bias: -267.424574, T: 8622792, Avg. loss: 314.515777\n",
            "Total training time: 26.72 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 10696.08, NNZs: 282, Bias: -267.426588, T: 8656740, Avg. loss: 316.546157\n",
            "Total training time: 26.83 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 10696.86, NNZs: 276, Bias: -267.423083, T: 8690688, Avg. loss: 316.349334\n",
            "Total training time: 26.95 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 10697.64, NNZs: 280, Bias: -267.424127, T: 8724636, Avg. loss: 316.221056\n",
            "Total training time: 27.06 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 10698.37, NNZs: 285, Bias: -267.412251, T: 8758584, Avg. loss: 327.929025\n",
            "Total training time: 27.18 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 10699.08, NNZs: 293, Bias: -267.386834, T: 8792532, Avg. loss: 325.957110\n",
            "Total training time: 27.29 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 10699.82, NNZs: 292, Bias: -267.371307, T: 8826480, Avg. loss: 316.176137\n",
            "Total training time: 27.40 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 10700.49, NNZs: 290, Bias: -267.363009, T: 8860428, Avg. loss: 337.745754\n",
            "Total training time: 27.52 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 10701.23, NNZs: 281, Bias: -267.359357, T: 8894376, Avg. loss: 310.021476\n",
            "Total training time: 27.63 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 10701.94, NNZs: 287, Bias: -267.345164, T: 8928324, Avg. loss: 316.801165\n",
            "Total training time: 27.74 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 10702.65, NNZs: 291, Bias: -267.337093, T: 8962272, Avg. loss: 311.250756\n",
            "Total training time: 27.86 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 10703.35, NNZs: 282, Bias: -267.315148, T: 8996220, Avg. loss: 317.952226\n",
            "Total training time: 27.97 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 10704.07, NNZs: 286, Bias: -267.320006, T: 9030168, Avg. loss: 306.575324\n",
            "Total training time: 28.08 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 10704.75, NNZs: 283, Bias: -267.310998, T: 9064116, Avg. loss: 312.964731\n",
            "Total training time: 28.20 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 10705.41, NNZs: 290, Bias: -267.309422, T: 9098064, Avg. loss: 314.335712\n",
            "Total training time: 28.31 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 10706.08, NNZs: 286, Bias: -267.302413, T: 9132012, Avg. loss: 320.363262\n",
            "Total training time: 28.43 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 10706.71, NNZs: 284, Bias: -267.284354, T: 9165960, Avg. loss: 324.052359\n",
            "Total training time: 28.54 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 10707.33, NNZs: 277, Bias: -267.260023, T: 9199908, Avg. loss: 316.532678\n",
            "Total training time: 28.65 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 10708.00, NNZs: 289, Bias: -267.258451, T: 9233856, Avg. loss: 300.074485\n",
            "Total training time: 28.76 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 10708.67, NNZs: 282, Bias: -267.250059, T: 9267804, Avg. loss: 308.190064\n",
            "Total training time: 28.87 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 10709.31, NNZs: 284, Bias: -267.227565, T: 9301752, Avg. loss: 306.380547\n",
            "Total training time: 28.99 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 10709.96, NNZs: 279, Bias: -267.212008, T: 9335700, Avg. loss: 299.685404\n",
            "Total training time: 29.10 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 10710.62, NNZs: 283, Bias: -267.196463, T: 9369648, Avg. loss: 291.004541\n",
            "Total training time: 29.21 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 10711.25, NNZs: 283, Bias: -267.184873, T: 9403596, Avg. loss: 301.290413\n",
            "Total training time: 29.32 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 10711.87, NNZs: 287, Bias: -267.176287, T: 9437544, Avg. loss: 302.836757\n",
            "Total training time: 29.43 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 10712.45, NNZs: 290, Bias: -267.191754, T: 9471492, Avg. loss: 311.266230\n",
            "Total training time: 29.54 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 10713.10, NNZs: 275, Bias: -267.183853, T: 9505440, Avg. loss: 290.365618\n",
            "Total training time: 29.66 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 10713.70, NNZs: 279, Bias: -267.163079, T: 9539388, Avg. loss: 292.013609\n",
            "Total training time: 29.77 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 10714.34, NNZs: 276, Bias: -267.151488, T: 9573336, Avg. loss: 294.000645\n",
            "Total training time: 29.88 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 10714.95, NNZs: 278, Bias: -267.149487, T: 9607284, Avg. loss: 294.229384\n",
            "Total training time: 30.00 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 10715.59, NNZs: 278, Bias: -267.131657, T: 9641232, Avg. loss: 282.849551\n",
            "Total training time: 30.11 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 10716.20, NNZs: 274, Bias: -267.105009, T: 9675180, Avg. loss: 286.764920\n",
            "Total training time: 30.22 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 10716.79, NNZs: 284, Bias: -267.087452, T: 9709128, Avg. loss: 278.544673\n",
            "Total training time: 30.34 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 10717.39, NNZs: 279, Bias: -267.080635, T: 9743076, Avg. loss: 280.472962\n",
            "Total training time: 30.45 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 10718.00, NNZs: 279, Bias: -267.056132, T: 9777024, Avg. loss: 277.762325\n",
            "Total training time: 30.56 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 10718.58, NNZs: 277, Bias: -267.046052, T: 9810972, Avg. loss: 293.582689\n",
            "Total training time: 30.68 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 10719.18, NNZs: 285, Bias: -267.040666, T: 9844920, Avg. loss: 273.845637\n",
            "Total training time: 30.79 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 10719.73, NNZs: 289, Bias: -267.015171, T: 9878868, Avg. loss: 290.605866\n",
            "Total training time: 30.91 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 10720.27, NNZs: 288, Bias: -267.002154, T: 9912816, Avg. loss: 290.077246\n",
            "Total training time: 31.02 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 10720.84, NNZs: 280, Bias: -266.980564, T: 9946764, Avg. loss: 290.676535\n",
            "Total training time: 31.14 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 10721.38, NNZs: 288, Bias: -266.984362, T: 9980712, Avg. loss: 286.631791\n",
            "Total training time: 31.25 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 10721.94, NNZs: 280, Bias: -266.966220, T: 10014660, Avg. loss: 286.329657\n",
            "Total training time: 31.36 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 10722.48, NNZs: 286, Bias: -266.947218, T: 10048608, Avg. loss: 282.073414\n",
            "Total training time: 31.47 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 10723.00, NNZs: 291, Bias: -266.939969, T: 10082556, Avg. loss: 285.841688\n",
            "Total training time: 31.58 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 10723.53, NNZs: 283, Bias: -266.933265, T: 10116504, Avg. loss: 283.673889\n",
            "Total training time: 31.70 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 10724.05, NNZs: 292, Bias: -266.913915, T: 10150452, Avg. loss: 284.710292\n",
            "Total training time: 31.81 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 10724.57, NNZs: 296, Bias: -266.896534, T: 10184400, Avg. loss: 285.292631\n",
            "Total training time: 31.93 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 10725.05, NNZs: 290, Bias: -266.877527, T: 10218348, Avg. loss: 288.279927\n",
            "Total training time: 32.04 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 10725.60, NNZs: 289, Bias: -266.855338, T: 10252296, Avg. loss: 275.623863\n",
            "Total training time: 32.16 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 10726.13, NNZs: 294, Bias: -266.831752, T: 10286244, Avg. loss: 278.089850\n",
            "Total training time: 32.27 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 10726.62, NNZs: 294, Bias: -266.806157, T: 10320192, Avg. loss: 286.614420\n",
            "Total training time: 32.39 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 10727.13, NNZs: 297, Bias: -266.810868, T: 10354140, Avg. loss: 276.734119\n",
            "Total training time: 32.51 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 10727.62, NNZs: 292, Bias: -266.811395, T: 10388088, Avg. loss: 281.204709\n",
            "Total training time: 32.62 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 10728.12, NNZs: 290, Bias: -266.796482, T: 10422036, Avg. loss: 276.129570\n",
            "Total training time: 32.72 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 10728.60, NNZs: 286, Bias: -266.790241, T: 10455984, Avg. loss: 267.593851\n",
            "Total training time: 32.83 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 10729.09, NNZs: 287, Bias: -266.758625, T: 10489932, Avg. loss: 271.017131\n",
            "Total training time: 32.93 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 10729.59, NNZs: 290, Bias: -266.739667, T: 10523880, Avg. loss: 265.159147\n",
            "Total training time: 33.04 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 10730.07, NNZs: 275, Bias: -266.718647, T: 10557828, Avg. loss: 279.620253\n",
            "Total training time: 33.14 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 10730.55, NNZs: 290, Bias: -266.676903, T: 10591776, Avg. loss: 273.736833\n",
            "Total training time: 33.25 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 10731.04, NNZs: 291, Bias: -266.650731, T: 10625724, Avg. loss: 264.104722\n",
            "Total training time: 33.35 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 10731.52, NNZs: 286, Bias: -266.627269, T: 10659672, Avg. loss: 267.408209\n",
            "Total training time: 33.46 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 10732.02, NNZs: 277, Bias: -266.604694, T: 10693620, Avg. loss: 262.803652\n",
            "Total training time: 33.56 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 10732.49, NNZs: 278, Bias: -266.593757, T: 10727568, Avg. loss: 264.924826\n",
            "Total training time: 33.67 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 10732.99, NNZs: 283, Bias: -266.584737, T: 10761516, Avg. loss: 258.828141\n",
            "Total training time: 33.77 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 10733.48, NNZs: 288, Bias: -266.569911, T: 10795464, Avg. loss: 260.231582\n",
            "Total training time: 33.88 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 10733.93, NNZs: 290, Bias: -266.544344, T: 10829412, Avg. loss: 262.552700\n",
            "Total training time: 33.98 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 10734.38, NNZs: 282, Bias: -266.526846, T: 10863360, Avg. loss: 267.280941\n",
            "Total training time: 34.09 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 10734.84, NNZs: 295, Bias: -266.490038, T: 10897308, Avg. loss: 260.450006\n",
            "Total training time: 34.20 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 10735.27, NNZs: 285, Bias: -266.473046, T: 10931256, Avg. loss: 264.646095\n",
            "Total training time: 34.30 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 10735.74, NNZs: 283, Bias: -266.443417, T: 10965204, Avg. loss: 249.700877\n",
            "Total training time: 34.41 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 10736.17, NNZs: 285, Bias: -266.423498, T: 10999152, Avg. loss: 264.474777\n",
            "Total training time: 34.51 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 10736.61, NNZs: 301, Bias: -266.407691, T: 11033100, Avg. loss: 259.910427\n",
            "Total training time: 34.62 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 10737.01, NNZs: 295, Bias: -266.383485, T: 11067048, Avg. loss: 260.432325\n",
            "Total training time: 34.72 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 10737.44, NNZs: 293, Bias: -266.352880, T: 11100996, Avg. loss: 254.682906\n",
            "Total training time: 34.83 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 10737.86, NNZs: 276, Bias: -266.325971, T: 11134944, Avg. loss: 253.895771\n",
            "Total training time: 34.93 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 10738.31, NNZs: 279, Bias: -266.291432, T: 11168892, Avg. loss: 250.481735\n",
            "Total training time: 35.04 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 10738.74, NNZs: 287, Bias: -266.267308, T: 11202840, Avg. loss: 252.407412\n",
            "Total training time: 35.15 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 10739.16, NNZs: 290, Bias: -266.244722, T: 11236788, Avg. loss: 258.201033\n",
            "Total training time: 35.25 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 10739.60, NNZs: 283, Bias: -266.215389, T: 11270736, Avg. loss: 251.666142\n",
            "Total training time: 35.36 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 10740.01, NNZs: 278, Bias: -266.191827, T: 11304684, Avg. loss: 253.438375\n",
            "Total training time: 35.47 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 10740.44, NNZs: 291, Bias: -266.156318, T: 11338632, Avg. loss: 244.883078\n",
            "Total training time: 35.57 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 10740.86, NNZs: 288, Bias: -266.137918, T: 11372580, Avg. loss: 248.671578\n",
            "Total training time: 35.68 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 10741.29, NNZs: 288, Bias: -266.108703, T: 11406528, Avg. loss: 246.069106\n",
            "Total training time: 35.78 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 10741.69, NNZs: 285, Bias: -266.085020, T: 11440476, Avg. loss: 255.985250\n",
            "Total training time: 35.89 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 10742.11, NNZs: 287, Bias: -266.057545, T: 11474424, Avg. loss: 241.252969\n",
            "Total training time: 35.99 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 10742.52, NNZs: 289, Bias: -266.039253, T: 11508372, Avg. loss: 252.032630\n",
            "Total training time: 36.10 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 10742.96, NNZs: 284, Bias: -266.029355, T: 11542320, Avg. loss: 230.653789\n",
            "Total training time: 36.21 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 10743.38, NNZs: 287, Bias: -266.003540, T: 11576268, Avg. loss: 238.644016\n",
            "Total training time: 36.32 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 10743.80, NNZs: 292, Bias: -265.981647, T: 11610216, Avg. loss: 241.943951\n",
            "Total training time: 36.42 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 10744.19, NNZs: 285, Bias: -265.968794, T: 11644164, Avg. loss: 247.052187\n",
            "Total training time: 36.53 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 10744.57, NNZs: 289, Bias: -265.930068, T: 11678112, Avg. loss: 246.307891\n",
            "Total training time: 36.64 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 10744.98, NNZs: 283, Bias: -265.896602, T: 11712060, Avg. loss: 235.095082\n",
            "Total training time: 36.75 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 10745.39, NNZs: 282, Bias: -265.878197, T: 11746008, Avg. loss: 240.962156\n",
            "Total training time: 36.86 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 10745.79, NNZs: 279, Bias: -265.855047, T: 11779956, Avg. loss: 239.533659\n",
            "Total training time: 36.96 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 10746.18, NNZs: 288, Bias: -265.825003, T: 11813904, Avg. loss: 238.068625\n",
            "Total training time: 37.08 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 10746.54, NNZs: 287, Bias: -265.804852, T: 11847852, Avg. loss: 249.980979\n",
            "Total training time: 37.18 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 10746.91, NNZs: 296, Bias: -265.801842, T: 11881800, Avg. loss: 237.249610\n",
            "Total training time: 37.29 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 10747.27, NNZs: 288, Bias: -265.781011, T: 11915748, Avg. loss: 247.303927\n",
            "Total training time: 37.40 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 10747.64, NNZs: 280, Bias: -265.753367, T: 11949696, Avg. loss: 236.396095\n",
            "Total training time: 37.51 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 10748.01, NNZs: 281, Bias: -265.727124, T: 11983644, Avg. loss: 238.164341\n",
            "Total training time: 37.61 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 10748.38, NNZs: 288, Bias: -265.700870, T: 12017592, Avg. loss: 238.318643\n",
            "Total training time: 37.72 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 10748.75, NNZs: 294, Bias: -265.682929, T: 12051540, Avg. loss: 242.418954\n",
            "Total training time: 37.83 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 10749.11, NNZs: 290, Bias: -265.658820, T: 12085488, Avg. loss: 237.720872\n",
            "Total training time: 37.93 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 10749.45, NNZs: 285, Bias: -265.630125, T: 12119436, Avg. loss: 243.131276\n",
            "Total training time: 38.04 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 10749.81, NNZs: 288, Bias: -265.597817, T: 12153384, Avg. loss: 234.945752\n",
            "Total training time: 38.14 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 10750.17, NNZs: 294, Bias: -265.568527, T: 12187332, Avg. loss: 233.759594\n",
            "Total training time: 38.25 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 10750.52, NNZs: 282, Bias: -265.543222, T: 12221280, Avg. loss: 234.830177\n",
            "Total training time: 38.36 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 10750.88, NNZs: 291, Bias: -265.518643, T: 12255228, Avg. loss: 233.509439\n",
            "Total training time: 38.46 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 10751.23, NNZs: 292, Bias: -265.500914, T: 12289176, Avg. loss: 239.642387\n",
            "Total training time: 38.57 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 10751.62, NNZs: 282, Bias: -265.468097, T: 12323124, Avg. loss: 222.562457\n",
            "Total training time: 38.68 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 10751.97, NNZs: 296, Bias: -265.435593, T: 12357072, Avg. loss: 230.881923\n",
            "Total training time: 38.78 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 10752.31, NNZs: 285, Bias: -265.410043, T: 12391020, Avg. loss: 231.256274\n",
            "Total training time: 38.89 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 10752.66, NNZs: 295, Bias: -265.380528, T: 12424968, Avg. loss: 229.471437\n",
            "Total training time: 38.99 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 10752.99, NNZs: 292, Bias: -265.357136, T: 12458916, Avg. loss: 234.491409\n",
            "Total training time: 39.10 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 10753.33, NNZs: 288, Bias: -265.323711, T: 12492864, Avg. loss: 231.524331\n",
            "Total training time: 39.21 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 10753.65, NNZs: 284, Bias: -265.304440, T: 12526812, Avg. loss: 227.701763\n",
            "Total training time: 39.31 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 10753.99, NNZs: 288, Bias: -265.272239, T: 12560760, Avg. loss: 227.268342\n",
            "Total training time: 39.42 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 10754.32, NNZs: 290, Bias: -265.246359, T: 12594708, Avg. loss: 231.149049\n",
            "Total training time: 39.52 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 10754.65, NNZs: 289, Bias: -265.224135, T: 12628656, Avg. loss: 230.409932\n",
            "Total training time: 39.63 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 10754.98, NNZs: 293, Bias: -265.200743, T: 12662604, Avg. loss: 221.972177\n",
            "Total training time: 39.74 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 10755.29, NNZs: 288, Bias: -265.170700, T: 12696552, Avg. loss: 234.606168\n",
            "Total training time: 39.84 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 10755.61, NNZs: 291, Bias: -265.144335, T: 12730500, Avg. loss: 226.885293\n",
            "Total training time: 39.94 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 10755.93, NNZs: 285, Bias: -265.120941, T: 12764448, Avg. loss: 229.115441\n",
            "Total training time: 40.05 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 10756.24, NNZs: 284, Bias: -265.083802, T: 12798396, Avg. loss: 220.819178\n",
            "Total training time: 40.16 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 10756.57, NNZs: 288, Bias: -265.064928, T: 12832344, Avg. loss: 224.423386\n",
            "Total training time: 40.27 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 10756.89, NNZs: 299, Bias: -265.047029, T: 12866292, Avg. loss: 221.395212\n",
            "Total training time: 40.37 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 10757.20, NNZs: 297, Bias: -265.007136, T: 12900240, Avg. loss: 221.847507\n",
            "Total training time: 40.48 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 10757.53, NNZs: 292, Bias: -264.968113, T: 12934188, Avg. loss: 215.785571\n",
            "Total training time: 40.59 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 10757.83, NNZs: 285, Bias: -264.935465, T: 12968136, Avg. loss: 221.682429\n",
            "Total training time: 40.69 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 10758.14, NNZs: 291, Bias: -264.909979, T: 13002084, Avg. loss: 223.149738\n",
            "Total training time: 40.80 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 10758.45, NNZs: 285, Bias: -264.888578, T: 13036032, Avg. loss: 214.480108\n",
            "Total training time: 40.91 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 10758.76, NNZs: 283, Bias: -264.862890, T: 13069980, Avg. loss: 209.191463\n",
            "Total training time: 41.01 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 10759.07, NNZs: 280, Bias: -264.833441, T: 13103928, Avg. loss: 211.612384\n",
            "Total training time: 41.12 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 10759.38, NNZs: 285, Bias: -264.780347, T: 13137876, Avg. loss: 212.027003\n",
            "Total training time: 41.23 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 10759.69, NNZs: 288, Bias: -264.753636, T: 13171824, Avg. loss: 210.523434\n",
            "Total training time: 41.33 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 10760.00, NNZs: 297, Bias: -264.728929, T: 13205772, Avg. loss: 209.818833\n",
            "Total training time: 41.44 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 10760.31, NNZs: 282, Bias: -264.705251, T: 13239720, Avg. loss: 215.221725\n",
            "Total training time: 41.55 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 10760.61, NNZs: 280, Bias: -264.675057, T: 13273668, Avg. loss: 211.093395\n",
            "Total training time: 41.65 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 10760.93, NNZs: 287, Bias: -264.641128, T: 13307616, Avg. loss: 200.560805\n",
            "Total training time: 41.76 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 10761.24, NNZs: 284, Bias: -264.616423, T: 13341564, Avg. loss: 208.934378\n",
            "Total training time: 41.87 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 10761.54, NNZs: 285, Bias: -264.586351, T: 13375512, Avg. loss: 205.144421\n",
            "Total training time: 41.97 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 10761.82, NNZs: 294, Bias: -264.555023, T: 13409460, Avg. loss: 219.511875\n",
            "Total training time: 42.08 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 10762.10, NNZs: 290, Bias: -264.534528, T: 13443408, Avg. loss: 213.517908\n",
            "Total training time: 42.19 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 10762.39, NNZs: 279, Bias: -264.499176, T: 13477356, Avg. loss: 213.404803\n",
            "Total training time: 42.29 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 10762.69, NNZs: 295, Bias: -264.468998, T: 13511304, Avg. loss: 209.753025\n",
            "Total training time: 42.40 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 10762.97, NNZs: 288, Bias: -264.436698, T: 13545252, Avg. loss: 207.001961\n",
            "Total training time: 42.51 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 10763.26, NNZs: 289, Bias: -264.413208, T: 13579200, Avg. loss: 210.434532\n",
            "Total training time: 42.61 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 10763.53, NNZs: 281, Bias: -264.383262, T: 13613148, Avg. loss: 212.386441\n",
            "Total training time: 42.72 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 10763.84, NNZs: 282, Bias: -264.348828, T: 13647096, Avg. loss: 201.745187\n",
            "Total training time: 42.83 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 10764.13, NNZs: 286, Bias: -264.315513, T: 13681044, Avg. loss: 201.674118\n",
            "Total training time: 42.94 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 10764.42, NNZs: 282, Bias: -264.277594, T: 13714992, Avg. loss: 199.040333\n",
            "Total training time: 43.05 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 10764.70, NNZs: 287, Bias: -264.240067, T: 13748940, Avg. loss: 200.024522\n",
            "Total training time: 43.16 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 10764.97, NNZs: 289, Bias: -264.210084, T: 13782888, Avg. loss: 209.111753\n",
            "Total training time: 43.27 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 10765.24, NNZs: 283, Bias: -264.176121, T: 13816836, Avg. loss: 201.738829\n",
            "Total training time: 43.38 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 10765.51, NNZs: 288, Bias: -264.158409, T: 13850784, Avg. loss: 200.797356\n",
            "Total training time: 43.49 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 10765.78, NNZs: 295, Bias: -264.127739, T: 13884732, Avg. loss: 211.090231\n",
            "Total training time: 43.60 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 10766.05, NNZs: 280, Bias: -264.103214, T: 13918680, Avg. loss: 203.444643\n",
            "Total training time: 43.71 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 10766.32, NNZs: 292, Bias: -264.055265, T: 13952628, Avg. loss: 202.010111\n",
            "Total training time: 43.81 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 10766.60, NNZs: 276, Bias: -264.015700, T: 13986576, Avg. loss: 193.131189\n",
            "Total training time: 43.92 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 10766.87, NNZs: 287, Bias: -263.979949, T: 14020524, Avg. loss: 197.447397\n",
            "Total training time: 44.03 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 10767.15, NNZs: 286, Bias: -263.938863, T: 14054472, Avg. loss: 190.059388\n",
            "Total training time: 44.14 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 10767.43, NNZs: 285, Bias: -263.904210, T: 14088420, Avg. loss: 187.250182\n",
            "Total training time: 44.25 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 10767.70, NNZs: 289, Bias: -263.882996, T: 14122368, Avg. loss: 191.506981\n",
            "Total training time: 44.36 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 10767.96, NNZs: 285, Bias: -263.847674, T: 14156316, Avg. loss: 192.973355\n",
            "Total training time: 44.47 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 10768.24, NNZs: 278, Bias: -263.804994, T: 14190264, Avg. loss: 186.990733\n",
            "Total training time: 44.58 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 10768.50, NNZs: 283, Bias: -263.767564, T: 14224212, Avg. loss: 198.285026\n",
            "Total training time: 44.68 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 10768.76, NNZs: 275, Bias: -263.742038, T: 14258160, Avg. loss: 192.695253\n",
            "Total training time: 44.79 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 10769.03, NNZs: 277, Bias: -263.712124, T: 14292108, Avg. loss: 188.999796\n",
            "Total training time: 44.90 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 10769.28, NNZs: 281, Bias: -263.680433, T: 14326056, Avg. loss: 193.218162\n",
            "Total training time: 45.01 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 10769.53, NNZs: 282, Bias: -263.648634, T: 14360004, Avg. loss: 193.557380\n",
            "Total training time: 45.11 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 10769.79, NNZs: 279, Bias: -263.616183, T: 14393952, Avg. loss: 187.674730\n",
            "Total training time: 45.22 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 10770.06, NNZs: 284, Bias: -263.575780, T: 14427900, Avg. loss: 188.223282\n",
            "Total training time: 45.33 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 10770.32, NNZs: 279, Bias: -263.549246, T: 14461848, Avg. loss: 183.458130\n",
            "Total training time: 45.44 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 10770.57, NNZs: 288, Bias: -263.521559, T: 14495796, Avg. loss: 186.117203\n",
            "Total training time: 45.54 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 10770.83, NNZs: 281, Bias: -263.503811, T: 14529744, Avg. loss: 187.997078\n",
            "Total training time: 45.65 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 10771.09, NNZs: 287, Bias: -263.468461, T: 14563692, Avg. loss: 186.882105\n",
            "Total training time: 45.76 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 10771.32, NNZs: 285, Bias: -263.434729, T: 14597640, Avg. loss: 200.615387\n",
            "Total training time: 45.87 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 10771.57, NNZs: 286, Bias: -263.409169, T: 14631588, Avg. loss: 184.476250\n",
            "Total training time: 45.97 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 10771.81, NNZs: 272, Bias: -263.369365, T: 14665536, Avg. loss: 189.369856\n",
            "Total training time: 46.08 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 10772.06, NNZs: 281, Bias: -263.341049, T: 14699484, Avg. loss: 188.514814\n",
            "Total training time: 46.19 seconds.\n",
            "-- Epoch 434\n",
            "Norm: 10772.31, NNZs: 284, Bias: -263.320073, T: 14733432, Avg. loss: 180.514616\n",
            "Total training time: 46.30 seconds.\n",
            "-- Epoch 435\n",
            "Norm: 10772.57, NNZs: 293, Bias: -263.298090, T: 14767380, Avg. loss: 185.274219\n",
            "Total training time: 46.40 seconds.\n",
            "-- Epoch 436\n",
            "Norm: 10772.82, NNZs: 282, Bias: -263.266492, T: 14801328, Avg. loss: 182.880154\n",
            "Total training time: 46.51 seconds.\n",
            "-- Epoch 437\n",
            "Norm: 10773.07, NNZs: 279, Bias: -263.230236, T: 14835276, Avg. loss: 176.013839\n",
            "Total training time: 46.62 seconds.\n",
            "-- Epoch 438\n",
            "Norm: 10773.32, NNZs: 287, Bias: -263.192494, T: 14869224, Avg. loss: 186.419766\n",
            "Total training time: 46.73 seconds.\n",
            "-- Epoch 439\n",
            "Norm: 10773.54, NNZs: 281, Bias: -263.153881, T: 14903172, Avg. loss: 190.514954\n",
            "Total training time: 46.83 seconds.\n",
            "-- Epoch 440\n",
            "Norm: 10773.77, NNZs: 283, Bias: -263.129969, T: 14937120, Avg. loss: 182.210202\n",
            "Total training time: 46.94 seconds.\n",
            "-- Epoch 441\n",
            "Norm: 10774.02, NNZs: 275, Bias: -263.091156, T: 14971068, Avg. loss: 171.911772\n",
            "Total training time: 47.05 seconds.\n",
            "-- Epoch 442\n",
            "Norm: 10774.25, NNZs: 291, Bias: -263.062396, T: 15005016, Avg. loss: 188.329566\n",
            "Total training time: 47.16 seconds.\n",
            "-- Epoch 443\n",
            "Norm: 10774.48, NNZs: 283, Bias: -263.032125, T: 15038964, Avg. loss: 186.527648\n",
            "Total training time: 47.26 seconds.\n",
            "-- Epoch 444\n",
            "Norm: 10774.72, NNZs: 282, Bias: -262.990358, T: 15072912, Avg. loss: 181.885952\n",
            "Total training time: 47.37 seconds.\n",
            "-- Epoch 445\n",
            "Norm: 10774.95, NNZs: 285, Bias: -262.964859, T: 15106860, Avg. loss: 187.416796\n",
            "Total training time: 47.48 seconds.\n",
            "-- Epoch 446\n",
            "Norm: 10775.18, NNZs: 272, Bias: -262.929814, T: 15140808, Avg. loss: 176.688964\n",
            "Total training time: 47.59 seconds.\n",
            "-- Epoch 447\n",
            "Norm: 10775.41, NNZs: 274, Bias: -262.902289, T: 15174756, Avg. loss: 180.878679\n",
            "Total training time: 47.69 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 10775.65, NNZs: 282, Bias: -262.865116, T: 15208704, Avg. loss: 174.918346\n",
            "Total training time: 47.81 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 10775.89, NNZs: 275, Bias: -262.837810, T: 15242652, Avg. loss: 182.066834\n",
            "Total training time: 47.92 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 10776.11, NNZs: 288, Bias: -262.798569, T: 15276600, Avg. loss: 180.585713\n",
            "Total training time: 48.04 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 10776.32, NNZs: 288, Bias: -262.762468, T: 15310548, Avg. loss: 186.101206\n",
            "Total training time: 48.15 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 10776.55, NNZs: 280, Bias: -262.725578, T: 15344496, Avg. loss: 178.325072\n",
            "Total training time: 48.26 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 10776.76, NNZs: 283, Bias: -262.688983, T: 15378444, Avg. loss: 183.558075\n",
            "Total training time: 48.37 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 10776.97, NNZs: 298, Bias: -262.654142, T: 15412392, Avg. loss: 189.305575\n",
            "Total training time: 48.49 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 10777.18, NNZs: 288, Bias: -262.621486, T: 15446340, Avg. loss: 185.380247\n",
            "Total training time: 48.60 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 10777.38, NNZs: 284, Bias: -262.593041, T: 15480288, Avg. loss: 188.432023\n",
            "Total training time: 48.72 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 10777.61, NNZs: 291, Bias: -262.555203, T: 15514236, Avg. loss: 177.756607\n",
            "Total training time: 48.83 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 10777.83, NNZs: 276, Bias: -262.511412, T: 15548184, Avg. loss: 175.179578\n",
            "Total training time: 48.94 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 10778.05, NNZs: 281, Bias: -262.482982, T: 15582132, Avg. loss: 177.348067\n",
            "Total training time: 49.06 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 10778.27, NNZs: 276, Bias: -262.442893, T: 15616080, Avg. loss: 176.476710\n",
            "Total training time: 49.17 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 10778.50, NNZs: 286, Bias: -262.410003, T: 15650028, Avg. loss: 167.294608\n",
            "Total training time: 49.29 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 10778.72, NNZs: 275, Bias: -262.377843, T: 15683976, Avg. loss: 168.571094\n",
            "Total training time: 49.41 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 10778.94, NNZs: 286, Bias: -262.347772, T: 15717924, Avg. loss: 170.403620\n",
            "Total training time: 49.52 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 10779.15, NNZs: 279, Bias: -262.312702, T: 15751872, Avg. loss: 176.825781\n",
            "Total training time: 49.64 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 10779.36, NNZs: 284, Bias: -262.276820, T: 15785820, Avg. loss: 174.160437\n",
            "Total training time: 49.76 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 10779.57, NNZs: 285, Bias: -262.239550, T: 15819768, Avg. loss: 171.122641\n",
            "Total training time: 49.87 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 10779.79, NNZs: 278, Bias: -262.203675, T: 15853716, Avg. loss: 170.410713\n",
            "Total training time: 49.99 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 10780.00, NNZs: 274, Bias: -262.171407, T: 15887664, Avg. loss: 170.421282\n",
            "Total training time: 50.10 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 10780.21, NNZs: 276, Bias: -262.140861, T: 15921612, Avg. loss: 165.672999\n",
            "Total training time: 50.21 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 10780.43, NNZs: 274, Bias: -262.100581, T: 15955560, Avg. loss: 170.497784\n",
            "Total training time: 50.33 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 10780.63, NNZs: 288, Bias: -262.064979, T: 15989508, Avg. loss: 170.313799\n",
            "Total training time: 50.45 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 10780.83, NNZs: 286, Bias: -262.021042, T: 16023456, Avg. loss: 171.554060\n",
            "Total training time: 50.56 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 10781.04, NNZs: 290, Bias: -261.982713, T: 16057404, Avg. loss: 170.767730\n",
            "Total training time: 50.68 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 10781.24, NNZs: 278, Bias: -261.937567, T: 16091352, Avg. loss: 166.870749\n",
            "Total training time: 50.79 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 10781.44, NNZs: 281, Bias: -261.906239, T: 16125300, Avg. loss: 169.542291\n",
            "Total training time: 50.90 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 10781.64, NNZs: 289, Bias: -261.876280, T: 16159248, Avg. loss: 174.379037\n",
            "Total training time: 51.02 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 10781.83, NNZs: 288, Bias: -261.837673, T: 16193196, Avg. loss: 169.789809\n",
            "Total training time: 51.13 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 10782.02, NNZs: 283, Bias: -261.812681, T: 16227144, Avg. loss: 173.287806\n",
            "Total training time: 51.25 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 10782.21, NNZs: 285, Bias: -261.779439, T: 16261092, Avg. loss: 176.397917\n",
            "Total training time: 51.36 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 10782.40, NNZs: 291, Bias: -261.736707, T: 16295040, Avg. loss: 167.521612\n",
            "Total training time: 51.48 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 10782.60, NNZs: 285, Bias: -261.703317, T: 16328988, Avg. loss: 168.621969\n",
            "Total training time: 51.60 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 10782.79, NNZs: 281, Bias: -261.665252, T: 16362936, Avg. loss: 169.982328\n",
            "Total training time: 51.71 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 10782.98, NNZs: 285, Bias: -261.627945, T: 16396884, Avg. loss: 169.375524\n",
            "Total training time: 51.83 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 10783.18, NNZs: 283, Bias: -261.592494, T: 16430832, Avg. loss: 168.628125\n",
            "Total training time: 51.95 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 10783.36, NNZs: 277, Bias: -261.553030, T: 16464780, Avg. loss: 169.690854\n",
            "Total training time: 52.06 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 10783.56, NNZs: 288, Bias: -261.514377, T: 16498728, Avg. loss: 169.201437\n",
            "Total training time: 52.18 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 10783.74, NNZs: 280, Bias: -261.483525, T: 16532676, Avg. loss: 163.290698\n",
            "Total training time: 52.30 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 10783.93, NNZs: 279, Bias: -261.440730, T: 16566624, Avg. loss: 162.520694\n",
            "Total training time: 52.42 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 10784.11, NNZs: 284, Bias: -261.416957, T: 16600572, Avg. loss: 165.403071\n",
            "Total training time: 52.53 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 10784.29, NNZs: 289, Bias: -261.384100, T: 16634520, Avg. loss: 166.956515\n",
            "Total training time: 52.65 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 10784.48, NNZs: 290, Bias: -261.353702, T: 16668468, Avg. loss: 165.964012\n",
            "Total training time: 52.77 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 10784.66, NNZs: 284, Bias: -261.318386, T: 16702416, Avg. loss: 165.990714\n",
            "Total training time: 52.88 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 10784.84, NNZs: 292, Bias: -261.281503, T: 16736364, Avg. loss: 170.733297\n",
            "Total training time: 52.99 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 10785.03, NNZs: 287, Bias: -261.243636, T: 16770312, Avg. loss: 163.353483\n",
            "Total training time: 53.11 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 10785.20, NNZs: 283, Bias: -261.204630, T: 16804260, Avg. loss: 165.811954\n",
            "Total training time: 53.22 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 10785.38, NNZs: 285, Bias: -261.170023, T: 16838208, Avg. loss: 162.210069\n",
            "Total training time: 53.33 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 10785.57, NNZs: 285, Bias: -261.132290, T: 16872156, Avg. loss: 158.926309\n",
            "Total training time: 53.44 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 10785.75, NNZs: 284, Bias: -261.094114, T: 16906104, Avg. loss: 163.920449\n",
            "Total training time: 53.54 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 10785.93, NNZs: 291, Bias: -261.057225, T: 16940052, Avg. loss: 168.158387\n",
            "Total training time: 53.65 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 10786.11, NNZs: 283, Bias: -261.017779, T: 16974000, Avg. loss: 161.666706\n",
            "Total training time: 53.76 seconds.\n",
            "0.2854982817869416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  8.8min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oczY2YaHRXK0",
        "colab_type": "code",
        "outputId": "1a81d77a-71a2-48c5-a0dd-aa101ae9275b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(solver='sag',multi_class='auto',verbose=1,max_iter=50)\n",
        "clf.fit(X_train, Y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print sklearn.metrics.accuracy_score(Y_test.values,pred)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 32 seconds\n",
            "0.3024742268041237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHa1xGeJSNzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K69dp6dFx8d",
        "colab_type": "code",
        "outputId": "e6aef2db-bd3b-4c14-dd8c-abad3287ca27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.square(np.array([1,2,3,4]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  4,  9, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ogi9BKzF2bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}